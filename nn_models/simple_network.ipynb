{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network to minimize CRPS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EMOS analog is a simple network like this:\n",
    "\n",
    "![title](EMOS_network.png)\n",
    "\n",
    "In this notebook we will build this simple network in theano and use the CRPS as a cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x10e59c8b8>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's import the libraries we need\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from importlib import reload   # So that we can reload utils\n",
    "import utils; reload(utils)   # This contains our own functions\n",
    "from utils import *\n",
    "\n",
    "# Let's make this notebook reproducible by defining the random seed\n",
    "np.random.RandomState(42)   # I don't even like the hitchhiker..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed this tutorial to figure out the theano basics: http://www.marekrei.com/blog/theano-tutorial/\n",
    "\n",
    "We will now attempt to build a simplle class for our model following: https://github.com/marekrei/theano-tutorial/blob/master/classifier.py\n",
    "\n",
    "So the first step is to create a class and initialize the network architecture. theano allocates a graph. This means that we first plot out the computations which will be done in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EMOS_Network(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This function is called once an object of this class is created.\n",
    "        \"\"\"\n",
    "        # Before we start with the network, let's define\n",
    "        # the learning rate as an input so we can vary it\n",
    "        lr = T.fscalar('lr')\n",
    "        \n",
    "        # First let's define the input to the network\n",
    "        # This is the ensemble mean (meanx), \n",
    "        # the ensemble stadnard deviation (stdx) and\n",
    "        # the corresponding observation (target)\n",
    "        # In theano we use tensors to describe these variables.\n",
    "        # T.fvector allocates a float32 1D vector\n",
    "        meanx = T.fvector('meanx')   # The name helps with debugging\n",
    "        stdx = T.fvector('stdx')\n",
    "        target = T.fvector('target')\n",
    "        \n",
    "        # Next we allocate the weights (a, b, c, d) as shared\n",
    "        # variables and initialize some value for them.\n",
    "        # For now we will just draw a random variable from N(0, 1)\n",
    "        a = theano.shared(np.random.randn(), 'a')\n",
    "        b = theano.shared(np.random.randn(), 'b')\n",
    "        c = theano.shared(np.random.randn(), 'c')\n",
    "        d = theano.shared(np.random.randn(), 'd')\n",
    "        \n",
    "        # Now that we have the input and the weights, \n",
    "        # we can set up the network.\n",
    "        mu = a + meanx * b\n",
    "        sigma = c + stdx * d\n",
    "        \n",
    "        # Now comes the cost function.\n",
    "        # To stop sigma from becoming negative we first have to \n",
    "        # convert it the the variance and then take the square\n",
    "        # root again. (I learned from experience...)\n",
    "        # This part of the code is inspired by Kai Polsterer's code!\n",
    "        var = T.sqr(sigma)\n",
    "        # The following three variables are just for convenience\n",
    "        loc = (target - mu) / T.sqrt(var)\n",
    "        phi = 1.0 / np.sqrt(2.0 * np.pi) * T.exp(-T.square(loc) / 2.0)\n",
    "        Phi = 0.5 * (1.0 + T.erf(loc / np.sqrt(2.0)))\n",
    "        # First we will compute the crps for each input/target pair\n",
    "        crps =  T.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
    "        # Then we take the mean. The cost is now a scalar\n",
    "        mean_crps = T.mean(crps)\n",
    "        \n",
    "        # Now compute the gradients of the cost function \n",
    "        # with respect to the four weights/parameters\n",
    "        params = [a, b, c, d]   # Let's put them in a list for convenience\n",
    "        gradients = theano.tensor.grad(mean_crps, params)\n",
    "        \n",
    "        # For gradient descent we now need to subtract the gradients\n",
    "        # from our parameters to minimize the cost function\n",
    "        # In theano we want to define a list of tuples containing\n",
    "        # the old parameter and the updated parameter.\n",
    "        updates = [(p, p - lr * g) for p, g in zip(params, gradients)]\n",
    "        \n",
    "        # So far no actual computations have been done. Now we will\n",
    "        # define a Theano function, which takes input, does some \n",
    "        # calculations and returns some output. In our case, we use \n",
    "        # meanx, stdx and the target as an input plus the required \n",
    "        # learning rate and return the mean_crps\n",
    "        # as an output. Then we tell the function to apply the update\n",
    "        # every time it is called. This is the training\n",
    "        self.train = theano.function([meanx, stdx, target, lr], \n",
    "                                     mean_crps, updates=updates)\n",
    "        # Furthermore, we define a method for simply making a prediction\n",
    "        # and returning the predicted values of mu and sigma\n",
    "        # along with the mean_crps without updating the parameters\n",
    "        self.predict = theano.function([meanx, stdx, target],\n",
    "                                       [mu, sigma, mean_crps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a sample prediction for one day. First we need to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = '/project/meteo/w2w/C7/ppnn_data/'   # At LMU\n",
    "DATA_DIR = '/Users/stephanrasp/repositories/ppnn/data/'  # Mac\n",
    "fn = 'data_interpolated.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the full dataset for 00 UTC\n",
    "tobs_full, tfc_full, dates = load_nc_data(DATA_DIR + fn, utc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's load the data for a particular date\n",
    "from datetime import datetime\n",
    "# Let's pick a random date\n",
    "date_idx = np.where(dates == datetime(2011, 2, 14, 0, 0))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_idx = 1507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get the data with the handy function in utils\n",
    "window_size = 50\n",
    "fclt = 48\n",
    "tfc_mean_train, tfc_std_train, tobs_train, \\\n",
    "    tfc_mean_test, tfc_std_test, tobs_test = \\\n",
    "        get_train_test_data(tobs_full, tfc_full, date_idx, \n",
    "                            window_size, fclt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's initialize the model\n",
    "model = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; mean_crps = 3.387\n",
      "Step 50; mean_crps = 1.598\n",
      "Step 100; mean_crps = 1.227\n",
      "Step 150; mean_crps = 1.178\n",
      "Step 200; mean_crps = 1.171\n",
      "Step 250; mean_crps = 1.169\n",
      "Step 300; mean_crps = 1.169\n",
      "Stop training at step 350\n"
     ]
    }
   ],
   "source": [
    "# And train it\n",
    "# Let's also implement early stopping\n",
    "early_stopping_delta = 0.000001\n",
    "lr = np.asarray(0.1, dtype='float32')\n",
    "cost_list = [1e99] * 5\n",
    "mean_cost_old = np.mean(cost_list[-5:])\n",
    "for i in range(10000):\n",
    "    cost = model.train(tfc_mean_train, tfc_std_train, tobs_train, lr)\n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    mean_cost = np.mean(cost_list[-5:])\n",
    "    if mean_cost_old - mean_cost < early_stopping_delta:\n",
    "        print('Stop training at step %i' % i)\n",
    "        break\n",
    "    \n",
    "    mean_cost_old = mean_cost\n",
    "    \n",
    "    if i%50 == 0: print('Step %i; mean_crps = %.3f' % (i, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the early stopping check whether the average of the last 5 training crps's still decreases. I think it's better to do this with a validation set, but we don't have one for now. For our simple network, this should be enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.0255328866675486)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tfc_mean_test, tfc_std_test, tobs_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1118336874995853"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compare that to the skill of the raw ensemble\n",
    "# Now rememeber tfc_mean_test and tfc_std_test are scaled,\n",
    "# so they are not compatible to tobs\n",
    "# So let's load the not-scaled (ns) data\n",
    "tobs_test, tfc_mean_test_ns, tfc_std_test_ns = prep_data(tobs_full[date_idx],\n",
    "                                                         tfc_full[date_idx])\n",
    "crps_normal(tfc_mean_test_ns, tfc_std_test_ns, tobs_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So a definite improvement! Let's now set up a loop to go through the period. Then let's check what takes time and maybe try running this on GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define the period to loop over\n",
    "# For testing purposes let's do 5 days\n",
    "from datetime import datetime\n",
    "date_idx_start = np.where(dates == datetime(2008, 1, 1, 0, 0))[0][0]\n",
    "date_idx_stop = np.where(dates == datetime(2008, 1, 31, 0, 0))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-01-01 00:00:00\n",
      "Stop training at step 964\n",
      "2008-01-02 00:00:00\n",
      "Stop training at step 32\n",
      "2008-01-03 00:00:00\n",
      "Stop training at step 42\n",
      "2008-01-04 00:00:00\n",
      "Stop training at step 54\n",
      "2008-01-05 00:00:00\n",
      "Stop training at step 74\n",
      "2008-01-06 00:00:00\n",
      "Stop training at step 73\n",
      "2008-01-07 00:00:00\n",
      "Stop training at step 66\n",
      "2008-01-08 00:00:00\n",
      "Stop training at step 15\n",
      "2008-01-09 00:00:00\n",
      "Stop training at step 55\n",
      "2008-01-10 00:00:00\n",
      "Stop training at step 33\n",
      "2008-01-11 00:00:00\n",
      "Stop training at step 61\n",
      "2008-01-12 00:00:00\n",
      "Stop training at step 15\n",
      "2008-01-13 00:00:00\n",
      "Stop training at step 45\n",
      "2008-01-14 00:00:00\n",
      "Stop training at step 54\n",
      "2008-01-15 00:00:00\n",
      "Stop training at step 38\n",
      "2008-01-16 00:00:00\n",
      "Stop training at step 33\n",
      "2008-01-17 00:00:00\n",
      "Stop training at step 36\n",
      "2008-01-18 00:00:00\n",
      "Stop training at step 38\n",
      "2008-01-19 00:00:00\n",
      "Stop training at step 73\n",
      "2008-01-20 00:00:00\n",
      "Stop training at step 71\n",
      "2008-01-21 00:00:00\n",
      "Stop training at step 75\n",
      "2008-01-22 00:00:00\n",
      "Stop training at step 83\n",
      "2008-01-23 00:00:00\n",
      "Stop training at step 68\n",
      "2008-01-24 00:00:00\n",
      "Stop training at step 70\n",
      "2008-01-25 00:00:00\n",
      "Stop training at step 23\n",
      "2008-01-26 00:00:00\n",
      "Stop training at step 66\n",
      "2008-01-27 00:00:00\n",
      "Stop training at step 36\n",
      "2008-01-28 00:00:00\n",
      "Stop training at step 79\n",
      "2008-01-29 00:00:00\n",
      "Stop training at step 84\n",
      "2008-01-30 00:00:00\n",
      "Stop training at step 29\n",
      "2008-01-31 00:00:00\n",
      "Stop training at step 47\n",
      "Time: 10.52 s\n",
      "0.18704517983132973\n",
      "0.9115673869964667\n",
      "9.410841987963067\n",
      "0.004909706010948867\n"
     ]
    }
   ],
   "source": [
    "# Loop\n",
    "import timeit\n",
    "crps_train_list = []\n",
    "crps_test_list = []\n",
    "lr = np.asarray(0.1, dtype='float32')\n",
    "window_size = 50\n",
    "nb_steps_max = 2000   # This is the maximum number of steps\n",
    "\n",
    "early_stopping_delta = 0.000001\n",
    "\n",
    "\n",
    "# Start timer\n",
    "time_start = timeit.default_timer()\n",
    "time_model = 0\n",
    "time_train = 0\n",
    "time_test = 0\n",
    "time_data = 0\n",
    "\n",
    "# Let's only initialize the model once, and then always update the weights\n",
    "t1 = timeit.default_timer()\n",
    "model = EMOS_Network()\n",
    "t2 = timeit.default_timer()\n",
    "time_model += t2 - t1\n",
    "\n",
    "for date_idx in range(date_idx_start, date_idx_stop + 1):\n",
    "    print(dates[date_idx])\n",
    "    # Get the data\n",
    "    t1 = timeit.default_timer()\n",
    "    tfc_mean_train, tfc_std_train, tobs_train, \\\n",
    "        tfc_mean_test, tfc_std_test, tobs_test = \\\n",
    "        get_train_test_data(tobs_full, tfc_full, date_idx, \n",
    "                            window_size, fclt)\n",
    "    t2 = timeit.default_timer()\n",
    "    time_data += t2 - t1\n",
    "    \n",
    "    # Train the model \n",
    "    t1 = timeit.default_timer()\n",
    "    cost_list = [1e99] * 5\n",
    "    mean_cost_old = np.mean(cost_list[-5:])\n",
    "    for i in range(nb_steps_max):\n",
    "        cost = model.train(tfc_mean_train, tfc_std_train, tobs_train, lr)\n",
    "        cost_list.append(cost)\n",
    "        mean_cost = np.mean(cost_list[-5:])\n",
    "        if mean_cost_old - mean_cost < early_stopping_delta:\n",
    "            print('Stop training at step %i' % i)\n",
    "            break\n",
    "        mean_cost_old = mean_cost\n",
    "    crps_train_list.append(cost)\n",
    "    \n",
    "    t2 = timeit.default_timer()\n",
    "    time_train += t2 - t1\n",
    "\n",
    "    # Now let's compute the CRPS of the test data\n",
    "    t1 = timeit.default_timer()\n",
    "    crps_pred = model.predict(tfc_mean_test, tfc_std_test, tobs_test)[2]\n",
    "    crps_test_list.append(crps_pred)\n",
    "    t2 = timeit.default_timer()\n",
    "    time_test += t2 - t1\n",
    "\n",
    "# Stop timer and print time\n",
    "time_stop = timeit.default_timer()\n",
    "print('Time: %.2f s' % (time_stop - time_start))\n",
    "print(time_data)\n",
    "print(time_model)\n",
    "print(time_train)\n",
    "print(time_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the early stopping and not reinitializing the model every step we can drastically reduce the training time. But let's check whether this actually gives the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
