{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network to minimize CRPS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EMOS analog is a simple network like this:\n",
    "\n",
    "![title](EMOS_network.png)\n",
    "\n",
    "In this notebook we will build this simple network in theano and use the CRPS as a cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x1126aeea0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's import the libraries we need\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "# Let's make this notebook reproducible by defining the random seed\n",
    "np.random.RandomState(42)   # I don't even like the hitchhiker..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed this tutorial to figure out the theano basics: http://www.marekrei.com/blog/theano-tutorial/\n",
    "\n",
    "We will now attempt to build a simplle class for our model following: https://github.com/marekrei/theano-tutorial/blob/master/classifier.py\n",
    "\n",
    "So the first step is to create a class and initialize the network architecture. theano allocates a graph. This means that we first plot out the computations which will be done in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EMOS_Network(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This function is called once an object of this class is created.\n",
    "        \"\"\"\n",
    "        # Before we start with the network, let's define\n",
    "        # the learning rate as an input so we can vary it\n",
    "        lr = T.fscalar('lr')\n",
    "        \n",
    "        # First let's define the input to the network\n",
    "        # This is the ensemble mean (meanx), \n",
    "        # the ensemble stadnard deviation (stdx) and\n",
    "        # the corresponding observation (target)\n",
    "        # In theano we use tensors to describe these variables.\n",
    "        # T.fvector allocates a float32 1D vector\n",
    "        meanx = T.fvector('meanx')   # The name helps with debugging\n",
    "        stdx = T.fvector('stdx')\n",
    "        target = T.fvector('target')\n",
    "        \n",
    "        # Next we allocate the weights (a, b, c, d) as shared\n",
    "        # variables and initialize some value for them.\n",
    "        # For now we will just draw a random variable from N(0, 1)\n",
    "        a = theano.shared(np.random.randn(), 'a')\n",
    "        b = theano.shared(np.random.randn(), 'b')\n",
    "        c = theano.shared(np.random.randn(), 'c')\n",
    "        d = theano.shared(np.random.randn(), 'd')\n",
    "        \n",
    "        # Now that we have the input and the weights, \n",
    "        # we can set up the network.\n",
    "        mu = a + meanx * b\n",
    "        sigma = c + stdx * d\n",
    "        \n",
    "        # Now comes the cost function.\n",
    "        # To stop sigma from becoming negative we first have to \n",
    "        # convert it the the variance and then take the square\n",
    "        # root again. (I learned from experience...)\n",
    "        # This part of the code is inspired by Kai Polsterer's code!\n",
    "        var = T.sqr(sigma)\n",
    "        # The following three variables are just for convenience\n",
    "        loc = (target - mu) / T.sqrt(var)\n",
    "        phi = 1.0 / np.sqrt(2.0 * np.pi) * T.exp(-T.square(loc) / 2.0)\n",
    "        Phi = 0.5 * (1.0 + T.erf(loc / np.sqrt(2.0)))\n",
    "        # First we will compute the crps for each input/target pair\n",
    "        crps =  T.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
    "        # Then we take the mean. The cost is now a scalar\n",
    "        mean_crps = T.mean(crps)\n",
    "        \n",
    "        # Now compute the gradients of the cost function \n",
    "        # with respect to the four weights/parameters\n",
    "        params = [a, b, c, d]   # Let's put them in a list for convenience\n",
    "        gradients = theano.tensor.grad(mean_crps, params)\n",
    "        \n",
    "        # For gradient descent we now need to subtract the gradients\n",
    "        # from our parameters to minimize the cost function\n",
    "        # In theano we want to define a list of tuples containing\n",
    "        # the old parameter and the updated parameter.\n",
    "        updates = [(p, p - lr * g) for p, g in zip(params, gradients)]\n",
    "        \n",
    "        # So far no actual computations have been done. Now we will\n",
    "        # define a Theano function, which takes input, does some \n",
    "        # calculations and returns some output. In our case, we use \n",
    "        # meanx, stdx and the target as an input plus the required \n",
    "        # learning rate and return the mean_crps\n",
    "        # as an output. Then we tell the function to apply the update\n",
    "        # every time it is called. This is the training\n",
    "        self.train = theano.function([meanx, stdx, target, lr], \n",
    "                                     mean_crps, updates=updates)\n",
    "        # Furthermore, we define a method for simply making a prediction\n",
    "        # and returning the predicted values of mu and sigma\n",
    "        # along with the mean_crps without updating the parameters\n",
    "        self.predict = theano.function([meanx, stdx, target],\n",
    "                                       [mu, sigma, mean_crps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define some input arrays\n",
    "nb_data = 100  # Number of data\n",
    "# It is important that the input has the same type (float32) as the Theano tensors\n",
    "in_meanx = np.asarray(np.random.randn(nb_data) + 3, dtype='float32')   # Random with mean 3 and std 1\n",
    "in_stdx = np.asarray(2 * np.random.randn(nb_data) + 1, dtype='float32')\n",
    "in_target = np.asarray(1.5 * np.random.randn(nb_data) + 2, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a perfect training set, where we should get perfect results\n",
    "in_meanx = np.ones(nb_data, dtype='float32') * 3\n",
    "in_stdx = np.ones(nb_data, dtype='float32') * 3\n",
    "in_target = np.ones(nb_data, dtype='float32') * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have set up our model and created some simple test data. Let's now initialize the network and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "model = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; mean_crps = 0.001\n",
      "Step 10; mean_crps = 0.002\n",
      "Step 20; mean_crps = 0.001\n",
      "Step 30; mean_crps = 0.007\n",
      "Step 40; mean_crps = 0.001\n",
      "Step 50; mean_crps = 0.000\n",
      "Step 60; mean_crps = 0.003\n",
      "Step 70; mean_crps = 0.001\n",
      "Step 80; mean_crps = 0.000\n",
      "Step 90; mean_crps = 0.001\n",
      "Step 100; mean_crps = 0.004\n",
      "Step 110; mean_crps = 0.001\n",
      "Step 120; mean_crps = 0.006\n",
      "Step 130; mean_crps = 0.001\n",
      "Step 140; mean_crps = 0.000\n",
      "Step 150; mean_crps = 0.000\n",
      "Step 160; mean_crps = 0.004\n",
      "Step 170; mean_crps = 0.000\n",
      "Step 180; mean_crps = 0.004\n",
      "Step 190; mean_crps = 0.001\n",
      "Step 200; mean_crps = 0.000\n",
      "Step 210; mean_crps = 0.002\n",
      "Step 220; mean_crps = 0.001\n",
      "Step 230; mean_crps = 0.002\n",
      "Step 240; mean_crps = 0.004\n",
      "Step 250; mean_crps = 0.001\n",
      "Step 260; mean_crps = 0.000\n",
      "Step 270; mean_crps = 0.001\n",
      "Step 280; mean_crps = 0.000\n",
      "Step 290; mean_crps = 0.001\n",
      "Step 300; mean_crps = 0.001\n",
      "Step 310; mean_crps = 0.001\n",
      "Step 320; mean_crps = 0.002\n",
      "Step 330; mean_crps = 0.003\n",
      "Step 340; mean_crps = 0.000\n",
      "Step 350; mean_crps = 0.004\n",
      "Step 360; mean_crps = 0.002\n",
      "Step 370; mean_crps = 0.000\n",
      "Step 380; mean_crps = 0.001\n",
      "Step 390; mean_crps = 0.002\n",
      "Step 400; mean_crps = 0.002\n",
      "Step 410; mean_crps = 0.000\n",
      "Step 420; mean_crps = 0.000\n",
      "Step 430; mean_crps = 0.002\n",
      "Step 440; mean_crps = 0.000\n",
      "Step 450; mean_crps = 0.001\n",
      "Step 460; mean_crps = 0.002\n",
      "Step 470; mean_crps = 0.001\n",
      "Step 480; mean_crps = 0.000\n",
      "Step 490; mean_crps = 0.001\n"
     ]
    }
   ],
   "source": [
    "# Let's run over the data a few times and print out the crps every few steps\n",
    "# Note that this is simply gradient descent, not stochastic, since\n",
    "# we are giving the algorithm all the data for each update/\n",
    "lr = np.asarray(0.001, dtype='float32')\n",
    "for i in range(500):\n",
    "    cost = model.train(in_meanx, in_stdx, in_target, lr)\n",
    "    if i%10 == 0: print('Step %i; mean_crps = %.3f' % (i, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(in_meanx, in_stdx, in_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.99962553,  1.99962553,  1.99962553,  1.99962553,  1.99962553])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good so far, we are able to reduce the CRPS and also get the correct predictions where possible. So now we can actually start thinking about real data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
