{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple network to minimize CRPS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EMOS analog is a simple network like this:\n",
    "\n",
    "![title](EMOS_network.png)\n",
    "\n",
    "In this notebook we will build this simple network in theano and use the CRPS as a cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x10e59c8b8>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's import the libraries we need\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from importlib import reload   # So that we can reload utils\n",
    "import utils; reload(utils)   # This contains our own functions\n",
    "from utils import *\n",
    "\n",
    "# Let's make this notebook reproducible by defining the random seed\n",
    "np.random.RandomState(42)   # I don't even like the hitchhiker..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed this tutorial to figure out the theano basics: http://www.marekrei.com/blog/theano-tutorial/\n",
    "\n",
    "We will now attempt to build a simplle class for our model following: https://github.com/marekrei/theano-tutorial/blob/master/classifier.py\n",
    "\n",
    "So the first step is to create a class and initialize the network architecture. theano allocates a graph. This means that we first plot out the computations which will be done in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EMOS_Network(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This function is called once an object of this class is created.\n",
    "        \"\"\"\n",
    "        # Before we start with the network, let's define\n",
    "        # the learning rate as an input so we can vary it\n",
    "        lr = T.fscalar('lr')\n",
    "        \n",
    "        # First let's define the input to the network\n",
    "        # This is the ensemble mean (meanx), \n",
    "        # the ensemble stadnard deviation (stdx) and\n",
    "        # the corresponding observation (target)\n",
    "        # In theano we use tensors to describe these variables.\n",
    "        # T.fvector allocates a float32 1D vector\n",
    "        meanx = T.fvector('meanx')   # The name helps with debugging\n",
    "        stdx = T.fvector('stdx')\n",
    "        target = T.fvector('target')\n",
    "        \n",
    "        # Next we allocate the weights (a, b, c, d) as shared\n",
    "        # variables and initialize some value for them.\n",
    "        # For now we will just draw a random variable from N(0, 1)\n",
    "        a = theano.shared(np.random.randn(), 'a')\n",
    "        b = theano.shared(np.random.randn(), 'b')\n",
    "        c = theano.shared(np.random.randn(), 'c')\n",
    "        d = theano.shared(np.random.randn(), 'd')\n",
    "        \n",
    "        # Now that we have the input and the weights, \n",
    "        # we can set up the network.\n",
    "        mu = a + meanx * b\n",
    "        sigma = c + stdx * d\n",
    "        \n",
    "        # Now comes the cost function.\n",
    "        # To stop sigma from becoming negative we first have to \n",
    "        # convert it the the variance and then take the square\n",
    "        # root again. (I learned from experience...)\n",
    "        # This part of the code is inspired by Kai Polsterer's code!\n",
    "        var = T.sqr(sigma)\n",
    "        # The following three variables are just for convenience\n",
    "        loc = (target - mu) / T.sqrt(var)\n",
    "        phi = 1.0 / np.sqrt(2.0 * np.pi) * T.exp(-T.square(loc) / 2.0)\n",
    "        Phi = 0.5 * (1.0 + T.erf(loc / np.sqrt(2.0)))\n",
    "        # First we will compute the crps for each input/target pair\n",
    "        crps =  T.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
    "        # Then we take the mean. The cost is now a scalar\n",
    "        mean_crps = T.mean(crps)\n",
    "        \n",
    "        # Now compute the gradients of the cost function \n",
    "        # with respect to the four weights/parameters\n",
    "        params = [a, b, c, d]   # Let's put them in a list for convenience\n",
    "        gradients = theano.tensor.grad(mean_crps, params)\n",
    "        \n",
    "        # For gradient descent we now need to subtract the gradients\n",
    "        # from our parameters to minimize the cost function\n",
    "        # In theano we want to define a list of tuples containing\n",
    "        # the old parameter and the updated parameter.\n",
    "        updates = [(p, p - lr * g) for p, g in zip(params, gradients)]\n",
    "        \n",
    "        # So far no actual computations have been done. Now we will\n",
    "        # define a Theano function, which takes input, does some \n",
    "        # calculations and returns some output. In our case, we use \n",
    "        # meanx, stdx and the target as an input plus the required \n",
    "        # learning rate and return the mean_crps\n",
    "        # as an output. Then we tell the function to apply the update\n",
    "        # every time it is called. This is the training\n",
    "        self.train = theano.function([meanx, stdx, target, lr], \n",
    "                                     mean_crps, updates=updates)\n",
    "        # Furthermore, we define a method for simply making a prediction\n",
    "        # and returning the predicted values of mu and sigma\n",
    "        # along with the mean_crps without updating the parameters\n",
    "        self.predict = theano.function([meanx, stdx, target],\n",
    "                                       [mu, sigma, mean_crps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a sample prediction for one day. First we need to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = '/project/meteo/w2w/C7/ppnn_data/'   # At LMU\n",
    "DATA_DIR = '/Users/stephanrasp/repositories/ppnn/data/'  # Mac\n",
    "fn = 'data_interpolated.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the full dataset for 00 UTC\n",
    "tobs_full, tfc_full, dates = load_nc_data(DATA_DIR + fn, utc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's load the data for a particular date\n",
    "from datetime import datetime\n",
    "# Let's pick a random date\n",
    "date_idx = np.where(dates == datetime(2011, 2, 14, 0, 0))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get the data with the handy function in utils\n",
    "window_size = 50\n",
    "fclt = 48\n",
    "tfc_mean_train, tfc_std_train, tobs_train, \\\n",
    "    tfc_mean_test, tfc_std_test, tobs_test = \\\n",
    "        get_train_test_data(tobs_full, tfc_full, date_idx, \n",
    "                            window_size, fclt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's initialize the model\n",
    "model = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; mean_crps = 3.167\n",
      "Step 20; mean_crps = 2.227\n",
      "Step 40; mean_crps = 1.668\n",
      "Step 60; mean_crps = 1.393\n",
      "Step 80; mean_crps = 1.272\n",
      "Step 100; mean_crps = 1.222\n",
      "Step 120; mean_crps = 1.201\n",
      "Step 140; mean_crps = 1.191\n",
      "Step 160; mean_crps = 1.187\n",
      "Step 180; mean_crps = 1.185\n",
      "Step 200; mean_crps = 1.184\n",
      "Step 220; mean_crps = 1.183\n",
      "Step 240; mean_crps = 1.183\n",
      "Step 260; mean_crps = 1.183\n",
      "Step 280; mean_crps = 1.183\n",
      "Step 300; mean_crps = 1.183\n",
      "Step 320; mean_crps = 1.183\n",
      "Step 340; mean_crps = 1.182\n",
      "Step 360; mean_crps = 1.182\n",
      "Step 380; mean_crps = 1.182\n",
      "Step 400; mean_crps = 1.182\n",
      "Step 420; mean_crps = 1.182\n",
      "Step 440; mean_crps = 1.182\n",
      "Step 460; mean_crps = 1.182\n",
      "Step 480; mean_crps = 1.182\n"
     ]
    }
   ],
   "source": [
    "# And train it\n",
    "lr = np.asarray(0.1, dtype='float32')\n",
    "for i in range(500):\n",
    "    cost = model.train(tfc_mean_train, tfc_std_train, tobs_train, lr)\n",
    "    if i%20 == 0: print('Step %i; mean_crps = %.3f' % (i, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that seems to work. For this case the CRPS stops decreasing after about 200 steps. Now let's make a prediction for the date we picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.77613490350798)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tfc_mean_test, tfc_std_test, tobs_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0212902697471031"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's compare that to the skill of the raw ensemble\n",
    "# Now rememeber tfc_mean_test and tfc_std_test are scaled,\n",
    "# so they are not compatible to tobs\n",
    "# So let's load the not-scaled (ns) data\n",
    "tobs_test, tfc_mean_test_ns, tfc_std_test_ns = prep_data(tobs_full[date_idx],\n",
    "                                                         tfc_full[date_idx])\n",
    "crps_normal(tfc_mean_test_ns, tfc_std_test_ns, tobs_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So a definite improvement! Let's now set up a loop to go through the period. Then let's check what takes time and maybe try running this on GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
