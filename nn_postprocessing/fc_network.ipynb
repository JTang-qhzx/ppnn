{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Fully-connected-linear-network\" data-toc-modified-id=\"Fully-connected-linear-network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Fully connected linear network</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Get-temperature-data\" data-toc-modified-id=\"Get-temperature-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get temperature data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Build-fully-connected-model\" data-toc-modified-id=\"Build-fully-connected-model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Build fully connected model</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Predict-for-one-day\" data-toc-modified-id=\"Predict-for-one-day-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Predict for one day</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Post-processing-with-rolling-window-for-2016\" data-toc-modified-id=\"Post-processing-with-rolling-window-for-2016-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Post processing with rolling window for 2016</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Train-2015,-predict-2016\" data-toc-modified-id=\"Train-2015,-predict-2016-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Train 2015, predict 2016</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Neural-network-with-one-hidden-layer\" data-toc-modified-id=\"Neural-network-with-one-hidden-layer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Neural network with one hidden layer</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Build-network\" data-toc-modified-id=\"Build-network-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Build network</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Train-2015,-predict-2016\" data-toc-modified-id=\"Train-2015,-predict-2016-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Train 2015, predict 2016</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Hidden-model-with-station-embeddings\" data-toc-modified-id=\"Hidden-model-with-station-embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hidden model with station embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Build-embedding-model\" data-toc-modified-id=\"Build-embedding-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Build embedding model</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Train-2015,-predict-2016\" data-toc-modified-id=\"Train-2015,-predict-2016-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Train 2015, predict 2016</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Embedding-size-hyper-parameter-tuning\" data-toc-modified-id=\"Embedding-size-hyper-parameter-tuning-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Embedding size hyper-parameter tuning</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Adding-additional-variables-to-hidden-layer-model\" data-toc-modified-id=\"Adding-additional-variables-to-hidden-layer-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Adding additional variables to hidden layer model</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Get-additional-variables\" data-toc-modified-id=\"Get-additional-variables-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Get additional variables</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Build-model-and-predict-for-2016\" data-toc-modified-id=\"Build-model-and-predict-for-2016-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Build model and predict for 2016</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Hidden-nodes-hyper-parameter-tuning\" data-toc-modified-id=\"Hidden-nodes-hyper-parameter-tuning-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Hidden nodes hyper-parameter tuning</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Additional-variables-with-the-embedding-model\" data-toc-modified-id=\"Additional-variables-with-the-embedding-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Additional variables with the embedding model</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/fc_network.ipynb#Build-model-and-predict-for-2016\" data-toc-modified-id=\"Build-model-and-predict-for-2016-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Build model and predict for 2016</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected neural networks\n",
    "\n",
    "In this notebook we will expand the simple EMOS linear network to fully connected non-linear neural nets. Furthermore, we will also use auxiliary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from importlib import reload\n",
    "import emos_network_theano; reload(emos_network_theano)\n",
    "from  emos_network_theano import EMOS_Network\n",
    "from crps_loss import crps_cost_function\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "# import the keras modules\n",
    "# Note that the cost function only works with the theano backend\n",
    "import keras\n",
    "from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "DATA_DIR = '/Volumes/STICK/data/ppnn_data/'  # Mac\n",
    "# DATA_DIR = '/project/meteo/w2w/C7/ppnn_data/'   # LMU\n",
    "results_dir = '../results/'\n",
    "window_size = 25   # Days in rolling window\n",
    "fclt = 48   # Forecast lead time in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected linear network\n",
    "\n",
    "As a first step, we can build a linear model which also connects the means and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get temperature data\n",
    "\n",
    "This follows the steps in the EMOS Network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 25 days\n",
      "test set contains 1 days\n"
     ]
    }
   ],
   "source": [
    "date_str = '2011-02-14'\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, predict_date=date_str,\n",
    "                                          fclt=fclt, window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build fully connected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fc_model():\n",
    "    inp = Input(shape=(2,))\n",
    "    x = Dense(2, activation='linear')(inp)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model()\n",
    "fc_model.compile(optimizer=SGD(0.1), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 6 parameters instead of 4 with the standard EMOS Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "early_stopping_delta = 1e-4   # How much the CRPS must improve before stopping\n",
    "steps_max = 1000   # How many steps to fit at max\n",
    "batch_size = train_set.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1249d8a20>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(train_set.features, train_set.targets, epochs=steps_max, \n",
    "             batch_size=batch_size,\n",
    "             validation_data=[test_set.features, test_set.targets], \n",
    "             verbose=0,\n",
    "             callbacks=[EarlyStopping(monitor='loss', \n",
    "                                      min_delta=early_stopping_delta,\n",
    "                                      patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.128275278515128, 0.76544715770375926)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get train and test CRPS\n",
    "(fc_model.evaluate(train_set.features, train_set.targets, batch_size, verbose=0), \n",
    " fc_model.evaluate(test_set.features, test_set.targets, batch_size, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular day we get a score that is slightly better than the standard EMOS network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing with rolling window for 2016\n",
    "\n",
    "As with the EMOS models let's do a rolling window global post-processing for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_str_start = '2016-01-01'\n",
    "date_str_stop = '2017-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model()\n",
    "fc_model.compile(optimizer=SGD(0.1), loss=crps_cost_function, \n",
    "                    metrics=[crps_cost_function])\n",
    "# Note that we have to define a metric in the corrent loop implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [07:54<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use the loop function in utils\n",
    "train_crps_list, valid_crps_list, results_df = loop_over_days(\n",
    "    DATA_DIR,\n",
    "    fc_model,\n",
    "    date_str_start, date_str_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=0.1,   \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98793579384560282, 1.0053749729681178)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_crps_list), np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get a slightly better training score and a slightly worse test score. This is a sign of overfitting. But the differences are small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(results_dir + 'fc_network_rolling_window.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2015, predict 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model()\n",
    "fc_model.compile(optimizer=Adam(0.1), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 3.2375 - val_loss: 2.1826\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.4673 - val_loss: 1.0894\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0780 - val_loss: 1.0150\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0694 - val_loss: 1.0116\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0692 - val_loss: 1.0126\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0692 - val_loss: 1.0118\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0692 - val_loss: 1.0121\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0692 - val_loss: 1.0122\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0124\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127bdfeb8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "             validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to the standard EMOS Network. This indicates that there is not much additional information in the two extra connections we added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fc_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3bbf78c4e2da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n\u001b[1;32m      3\u001b[0m                                preds[:, 0], preds[:, 1])\n\u001b[1;32m      4\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'fc_network_train_2015_pred_2016.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fc_model' is not defined"
     ]
    }
   ],
   "source": [
    "preds = fc_model.predict(test_set.features)\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'fc_network_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with one hidden layer\n",
    "\n",
    "Now we will build the first neural network with a hidden layer and a non-linear activation function. We will restrict ourselves to testing the 2015 training, 2016 prediction case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "hidden_nodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_hidden_model(hidden_nodes, feature_size=2):\n",
    "    inp = Input(shape=(feature_size,))\n",
    "    x = Dense(hidden_nodes, activation='relu')(inp)\n",
    "    x = Dense(2, activation='linear')(x)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 52\n",
      "Trainable params: 52\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_model = build_hidden_model(hidden_nodes)\n",
    "hidden_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_model.compile(optimizer=Adam(0.01), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2015, predict 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0705 - val_loss: 1.0128\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0700 - val_loss: 1.0142\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0709 - val_loss: 1.0144\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0699 - val_loss: 1.0140\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0705 - val_loss: 1.0116\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0711 - val_loss: 1.0136\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0703 - val_loss: 1.0152\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0702 - val_loss: 1.0134\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0701 - val_loss: 1.0129\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0703 - val_loss: 1.0148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127fdeb00>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the same data from above!\n",
    "hidden_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "                 validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss is consistently smaller than the validation loss. This indicates overfitting. We could use Dropout for regularization. But since this model is just an intermediate step anyway, we will ignore that for now. The differences are small anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = hidden_model.predict(test_set.features)\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'hidden_nn_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden model with station embeddings\n",
    "\n",
    "Next we will add a station embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_emb_model(hidden_nodes, emb_size, max_id, feature_size=2):\n",
    "    features_in = Input(shape=(feature_size,))\n",
    "    id_in = Input(shape=(1,))\n",
    "    emb = Embedding(max_id + 1, emb_size)(id_in)\n",
    "    emb = Flatten()(emb)\n",
    "    x = Concatenate()([features_in, emb])\n",
    "    x = Dense(hidden_nodes, activation='relu')(x)\n",
    "    x = Dense(2, activation='linear')(x)\n",
    "    model = Model(inputs=[features_in, id_in], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 536)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 5\n",
    "max_id = int(np.max([train_set.cont_ids.max(), test_set.cont_ids.max()]))\n",
    "hidden_nodes, max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 5)          2685        input_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 5)             0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 7)             0           input_10[0][0]                   \n",
      "                                                                   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 10)            80          concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 2)             22          dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2,787\n",
      "Trainable params: 2,787\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model = build_emb_model(hidden_nodes, emb_size, max_id)\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model.compile(optimizer=Adam(0.01), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2015, predict 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 3.6400 - val_loss: 1.3108\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0037 - val_loss: 0.9209\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9754 - val_loss: 0.9171\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9723 - val_loss: 0.9148\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9724 - val_loss: 0.9157\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9710 - val_loss: 0.9163\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9713 - val_loss: 0.9192\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9713 - val_loss: 0.9137\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9708 - val_loss: 0.9141\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9703 - val_loss: 0.9125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1286f6898>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, \n",
    "              epochs=10, batch_size=1024, \n",
    "              validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = emb_model.predict([test_set.features, test_set.cont_ids])\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'embedding_nn_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding size hyper-parameter tuning\n",
    "\n",
    "Since embeddings appear to work very well, we will test the impact of the embedding size before building more complex models. Of course, a larger embedding size might be useful when adding more variables, but this should give us some feeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_run_emb_model(emb_size):\n",
    "    emb_model = build_emb_model(hidden_nodes, emb_size, max_id)\n",
    "    emb_model.compile(optimizer=Adam(0.01), loss=crps_cost_function)\n",
    "    emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, \n",
    "                  epochs=20,batch_size=1024, verbose=0,\n",
    "                  validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])\n",
    "    print(emb_model.evaluate([train_set.features, train_set.cont_ids], train_set.targets, verbose=0),\n",
    "          emb_model.evaluate([test_set.features, test_set.cont_ids], test_set.targets, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.968565424108 0.929144545723\n",
      "2\n",
      "0.960039174862 0.919980559779\n",
      "3\n",
      "0.966939299072 0.914936043469\n",
      "5\n",
      "0.957627968952 0.926167305927\n",
      "10\n",
      "0.956032127737 0.92702534406\n",
      "20\n",
      "0.968370541059 0.920356115977\n"
     ]
    }
   ],
   "source": [
    "for emb_size in [1, 2, 3, 5, 10, 20]:\n",
    "    print(emb_size)\n",
    "    build_and_run_emb_model(emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is some variability. In our first experiment above with an embedding size of 5 we got a better score than here. For this very simple network an embedding size of two seems sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding additional variables to hidden layer model\n",
    "\n",
    "Now we can try adding additional variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get additional variables\n",
    "\n",
    "Using the function defined in utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The prepare_data function takes an ordered dict as an input\n",
    "aux_dict = OrderedDict()\n",
    "aux_dict['data_aux_geo_interpolated.nc'] = ['orog', \n",
    "                                            'station_alt', \n",
    "                                            'station_lat', \n",
    "                                            'station_lon']\n",
    "aux_dict['data_aux_pl500_interpolated_00UTC.nc'] = ['u_pl500_fc',\n",
    "                                                    'v_pl500_fc',\n",
    "                                                    'gh_pl500_fc']\n",
    "aux_dict['data_aux_pl850_interpolated_00UTC.nc'] = ['u_pl850_fc',\n",
    "                                                    'v_pl850_fc',\n",
    "                                                    'q_pl850_fc']\n",
    "aux_dict['data_aux_surface_interpolated_00UTC.nc'] = ['cape_fc',\n",
    "                                                      'sp_fc',\n",
    "                                                      'tcc_fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates,\n",
    "                                         aux_dict=aux_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180849, 24)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and predict for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_model = build_hidden_model(hidden_nodes, feature_size=train_set.features.shape[1])\n",
    "hidden_model.compile(optimizer=Adam(0.001), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that I had to reduce the learning rate here. Otherwise I seem to get stuck in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 10)                250       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 272\n",
      "Trainable params: 272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9508 - val_loss: 0.9414\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9503 - val_loss: 0.9391\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9500 - val_loss: 0.9395\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9494 - val_loss: 0.9405\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9490 - val_loss: 0.9404\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9485 - val_loss: 0.9397\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9482 - val_loss: 0.9403\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9482 - val_loss: 0.9393\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9474 - val_loss: 0.9408\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9471 - val_loss: 0.9411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e383400>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that I am running this cell multiple times\n",
    "hidden_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "                 validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see a definite improvement using auxiliary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden nodes hyper-parameter tuning\n",
    "\n",
    "Let's see what the impact of the number of hidden nodes is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_run_hidden_model(hidden_nodes):\n",
    "    hidden_model = build_hidden_model(hidden_nodes, feature_size=train_set.features.shape[1])\n",
    "    hidden_model.compile(optimizer=Adam(0.001), loss=crps_cost_function)\n",
    "    hidden_model.fit(train_set.features, train_set.targets, epochs=50, \n",
    "                  batch_size=1024, verbose=0,\n",
    "                  validation_data=[test_set.features, test_set.targets])\n",
    "    print(hidden_model.evaluate(train_set.features, train_set.targets, verbose=0),\n",
    "          hidden_model.evaluate(test_set.features, test_set.targets, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0.966121331424 0.93475101895\n",
      "25\n",
      "0.942864599186 0.933350656875\n",
      "50\n",
      "0.934860278494 0.933259091651\n",
      "100\n",
      "0.905984635154 0.939907191921\n",
      "250\n",
      "0.879771149796 0.933663448304\n",
      "1000\n",
      "0.850411047073 0.952754239677\n"
     ]
    }
   ],
   "source": [
    "for hidden_nodes in [10, 25, 50, 100, 250, 1000]:\n",
    "    print(hidden_nodes)\n",
    "    build_and_run_hidden_model(hidden_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with a larger network we get serious overfitting. Again, I would think that adding regularization with a larger network would maybe give the best results. Something to explore later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make a submission with 50 hidden nodes\n",
    "hidden_model = build_hidden_model(hidden_nodes=50, feature_size=train_set.features.shape[1])\n",
    "hidden_model.compile(optimizer=Adam(0.001), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9500 - val_loss: 0.9382\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9491 - val_loss: 0.9357\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9468 - val_loss: 0.9364\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9456 - val_loss: 0.9398\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9441 - val_loss: 0.9357\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9429 - val_loss: 0.9364\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9418 - val_loss: 0.9372\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9407 - val_loss: 0.9344\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9396 - val_loss: 0.9364\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9393 - val_loss: 0.9370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12191c828>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that I am running this cell multiple times\n",
    "hidden_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "                 validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = hidden_model.predict(test_set.features)\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'hidden_nn_aux_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional variables with the embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and predict for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes, emb_size, feature_size = 50, 5, train_set.features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_41 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 1, 5)          2685        input_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_40 (InputLayer)            (None, 24)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)              (None, 5)             0           embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 29)            0           input_40[0][0]                   \n",
      "                                                                   flatten_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_55 (Dense)                 (None, 50)            1500        concatenate_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_56 (Dense)                 (None, 2)             102         dense_55[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4,287\n",
      "Trainable params: 4,287\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model = build_emb_model(hidden_nodes, emb_size, max_id, feature_size)\n",
    "emb_model.compile(optimizer=Adam(0.001), loss=crps_cost_function)\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8485 - val_loss: 0.8541\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8475 - val_loss: 0.8543\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8459 - val_loss: 0.8559\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8449 - val_loss: 0.8562\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8439 - val_loss: 0.8573\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 1s - loss: 0.8434 - val_loss: 0.8571\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 1s - loss: 0.8418 - val_loss: 0.8540\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8412 - val_loss: 0.8611\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8404 - val_loss: 0.8557\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8392 - val_loss: 0.8579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120856198>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again I am running this multiple times\n",
    "emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, epochs=10, \n",
    "              batch_size=1024, \n",
    "              validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = emb_model.predict([test_set.features, test_set.cont_ids])\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'embedding_nn_aux_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
