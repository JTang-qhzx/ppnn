{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Prepare-the-data\" data-toc-modified-id=\"Prepare-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prepare the data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Theano-Implementation\" data-toc-modified-id=\"Theano-Implementation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Theano Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Train-for-a-single-day\" data-toc-modified-id=\"Train-for-a-single-day-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Train for a single day</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Post-processing-for-all-of-2016\" data-toc-modified-id=\"Post-processing-for-all-of-2016-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Post processing for all of 2016</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Post-processing-for-2016---benchmark-for-later-models\" data-toc-modified-id=\"Post-processing-for-2016---benchmark-for-later-models-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Post-processing for 2016 - benchmark for later models</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Keras-implementation\" data-toc-modified-id=\"Keras-implementation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Keras implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Predict-for-one-day\" data-toc-modified-id=\"Predict-for-one-day-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Predict for one day</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Post-processing-for-2016\" data-toc-modified-id=\"Post-processing-for-2016-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Post-processing for 2016</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Train-2015,-predict-2016\" data-toc-modified-id=\"Train-2015,-predict-2016-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Train 2015, predict 2016</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMOS Network\n",
    "\n",
    "The goal of this notebooks is to build and test a network implementation of EMOS, once in pure theano and then in keras. First we will try to replicate the results of the standard global EMOS. \n",
    "\n",
    "The reference period is always the mean CRPS for all dates and stations in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Note that the cost function only works with the theano backend for keras\n",
    "from importlib import reload\n",
    "import emos_network_theano; reload(emos_network_theano)\n",
    "from  emos_network_theano import EMOS_Network\n",
    "from crps_loss import crps_cost_function\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "DATA_DIR = '/Volumes/STICK/data/ppnn_data/'  # Mac\n",
    "# DATA_DIR = '/project/meteo/w2w/C7/ppnn_data/'   # LMU\n",
    "window_size = 25   # Days in rolling window\n",
    "fclt = 48   # Forecast lead time in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "The interpolated data set contains the observations and forecasts from the 50 members for each station and each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Chose a random date to illustrate the algorithm\n",
    "date_str = '2011-02-14'   # This is our standard date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 25 days\n",
      "test set contains 1 days\n",
      "Time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "# Load training and test set\n",
    "time_start = timeit.default_timer()\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, predict_date=date_str,\n",
    "                                          fclt=fclt, window_size=window_size)\n",
    "time_stop = timeit.default_timer()\n",
    "print('Time: %.2f s' % (time_stop - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preload dataset \n",
    "raw_data = load_raw_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 25 days\n",
      "test set contains 1 days\n",
      "Time: 0.89 s\n"
     ]
    }
   ],
   "source": [
    "# Check how long it takes with preloaded data\n",
    "import timeit\n",
    "time_start = timeit.default_timer()\n",
    "train_set, test_set = get_train_test_sets(preloaded_data=raw_data, predict_date=date_str,\n",
    "                                          fclt=fclt, window_size=window_size)\n",
    "time_stop = timeit.default_timer()\n",
    "print('Time: %.2f s' % (time_stop - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12619, 2), (12619,), (12619,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.shape, train_set.targets.shape, train_set.date_strs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((503, 2), (503,), (503,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.features.shape, test_set.targets.shape, test_set.date_strs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not up to date anymore\n",
    "\n",
    "Now we convert this dataset to use as the input for the EMOS networks. For this we pick a forecast date, and return as the training data all previous days within the window size previous to the start of the forecast. The test data is simply the data for the chosen forecast date. The 50 member ensemble is summarized by the mean and the standard deviation. Additionally, we remove all data where the observations are missing. Finally, the inputs are scaled. For this we subtract the mean from the mean and divide both the mean and the standard deviation by their standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These 1D arrays contain the data for all dates and stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano Implementation\n",
    "\n",
    "To start with we build the model in pure theano. The model is defined in a separate script `EMOS_network_theano.py`. The network uses a custom CRPS loss function which is defined in `crps_loss.py`.\n",
    "\n",
    "The EMOS_Network class is build to work in a similar way to keras models. For the fitting we are using gradient descent. Since we are using the entire dataset for each update, it is not stochastic. An early stopping algorithm is built into the fitting function. It stops training if the average training CRPS of the last 5 steps is decreasing by less than a parameter delta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train for a single day\n",
    "\n",
    "To illustrate how the model work we will use the data for our example day above and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define some model parameters\n",
    "lr = np.asarray(0.1, dtype='float32')   # The learning rate\n",
    "early_stopping_delta = 1e-4   # How much the CRPS must improve before stopping\n",
    "steps_max = 1000   # How many steps to fit at max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set up the theano model\n",
    "model_theano = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t2m_fc_mean', 't2m_fc_std']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the features into means and standard deviation\n",
    "train_set.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_mean = train_set.features[:, 0]\n",
    "train_std = train_set.features[:, 1]\n",
    "test_mean = test_set.features[:, 0]\n",
    "test_std = test_set.features[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1.1442610225440049), array(0.7698749147054791))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for some steps\n",
    "model_theano.fit(train_mean, train_std, train_set.targets, steps_max, \n",
    "                 (test_mean, test_std, test_set.targets), lr=lr, \n",
    "                 early_stopping_delta=early_stopping_delta)\n",
    "# Output is the training CRPS and the test CRPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing for all of 2016\n",
    "\n",
    "To compare the network model with the standard EMOS we will run it from 1 January 2016 to 31 December 2016. When looping over the days we are not resetting the model weights for each day. This drastically reduces the training time with identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get start and stop indices\n",
    "date_str_start = '2016-01-01'\n",
    "date_str_stop = '2016-01-02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_theano = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# This function loops over the days.\n",
    "train_crps_list, valid_crps_list, results_df = loop_over_days(\n",
    "    DATA_DIR,\n",
    "    model_theano,\n",
    "    date_str_start, date_str_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=lr,\n",
    "    verbose=0,\n",
    "    model_type='EMOS_Network_theano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>mean</th>\n",
       "      <th>station_id</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.691169</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.759432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.857408</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2.168383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.820915</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.826411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.646124</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.786254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.189787</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.256217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>6.651888</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.629443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.229099</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.859741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.055145</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.907856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.884506</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.890468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.781070</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2.300610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.365019</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.813105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.920648</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.841233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.280353</td>\n",
       "      <td>161.0</td>\n",
       "      <td>2.308564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.692707</td>\n",
       "      <td>164.0</td>\n",
       "      <td>2.007584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.903952</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1.980034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.683164</td>\n",
       "      <td>183.0</td>\n",
       "      <td>1.921277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.006185</td>\n",
       "      <td>191.0</td>\n",
       "      <td>2.201644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.463034</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1.971149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.382456</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.160858</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1.896019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.799459</td>\n",
       "      <td>232.0</td>\n",
       "      <td>1.934103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.689259</td>\n",
       "      <td>257.0</td>\n",
       "      <td>2.177241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.854178</td>\n",
       "      <td>259.0</td>\n",
       "      <td>2.199519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.445617</td>\n",
       "      <td>282.0</td>\n",
       "      <td>1.963576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.873723</td>\n",
       "      <td>294.0</td>\n",
       "      <td>1.889528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.410298</td>\n",
       "      <td>298.0</td>\n",
       "      <td>2.038168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.029109</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2.126704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1.895892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.621561</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1.843713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.830342</td>\n",
       "      <td>330.0</td>\n",
       "      <td>2.189949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.089389</td>\n",
       "      <td>7370.0</td>\n",
       "      <td>1.845518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>6.693358</td>\n",
       "      <td>7373.0</td>\n",
       "      <td>1.655053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.970713</td>\n",
       "      <td>7374.0</td>\n",
       "      <td>1.768615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.193146</td>\n",
       "      <td>7389.0</td>\n",
       "      <td>2.035943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.451519</td>\n",
       "      <td>7393.0</td>\n",
       "      <td>2.032126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.978763</td>\n",
       "      <td>7394.0</td>\n",
       "      <td>1.846960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.403638</td>\n",
       "      <td>7395.0</td>\n",
       "      <td>1.794460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.178230</td>\n",
       "      <td>7396.0</td>\n",
       "      <td>2.296881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.219952</td>\n",
       "      <td>7403.0</td>\n",
       "      <td>1.836180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.722299</td>\n",
       "      <td>7410.0</td>\n",
       "      <td>2.327132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.699415</td>\n",
       "      <td>7412.0</td>\n",
       "      <td>2.248362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.212402</td>\n",
       "      <td>7419.0</td>\n",
       "      <td>1.775764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.081976</td>\n",
       "      <td>7420.0</td>\n",
       "      <td>1.871855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.280326</td>\n",
       "      <td>7424.0</td>\n",
       "      <td>1.727813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.453304</td>\n",
       "      <td>7427.0</td>\n",
       "      <td>1.922315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>1.692605</td>\n",
       "      <td>7428.0</td>\n",
       "      <td>1.917260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.972521</td>\n",
       "      <td>7431.0</td>\n",
       "      <td>1.909445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.363104</td>\n",
       "      <td>7432.0</td>\n",
       "      <td>1.943679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>5.021493</td>\n",
       "      <td>13670.0</td>\n",
       "      <td>1.789755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.988470</td>\n",
       "      <td>13674.0</td>\n",
       "      <td>2.155038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>3.241753</td>\n",
       "      <td>13675.0</td>\n",
       "      <td>1.928294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.483021</td>\n",
       "      <td>13696.0</td>\n",
       "      <td>1.798250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.071138</td>\n",
       "      <td>13700.0</td>\n",
       "      <td>1.847649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.167720</td>\n",
       "      <td>13710.0</td>\n",
       "      <td>1.782137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.948429</td>\n",
       "      <td>13711.0</td>\n",
       "      <td>1.796199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.968321</td>\n",
       "      <td>13713.0</td>\n",
       "      <td>1.936687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.128452</td>\n",
       "      <td>13777.0</td>\n",
       "      <td>1.982405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>4.884018</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>1.821928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.303123</td>\n",
       "      <td>15207.0</td>\n",
       "      <td>2.124604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2.519523</td>\n",
       "      <td>15444.0</td>\n",
       "      <td>2.059423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date      mean  station_id       std\n",
       "0    2016-01-01  4.691169        44.0  1.759432\n",
       "1    2016-01-01  1.857408        71.0  2.168383\n",
       "2    2016-01-01  0.820915        73.0  1.826411\n",
       "3    2016-01-01  4.646124        78.0  1.786254\n",
       "4    2016-01-01  2.189787        91.0  2.256217\n",
       "5    2016-01-01  6.651888       102.0  1.629443\n",
       "6    2016-01-01  2.229099       125.0  1.859741\n",
       "7    2016-01-01  4.055145       131.0  1.907856\n",
       "8    2016-01-01  2.884506       142.0  1.890468\n",
       "9    2016-01-01  2.781070       150.0  2.300610\n",
       "10   2016-01-01  2.365019       151.0  1.813105\n",
       "11   2016-01-01  2.920648       154.0  1.841233\n",
       "12   2016-01-01  3.280353       161.0  2.308564\n",
       "13   2016-01-01  0.692707       164.0  2.007584\n",
       "14   2016-01-01  0.903952       167.0  1.980034\n",
       "15   2016-01-01  3.683164       183.0  1.921277\n",
       "16   2016-01-01  3.006185       191.0  2.201644\n",
       "17   2016-01-01  3.463034       198.0  1.971149\n",
       "18   2016-01-01  2.382456       217.0  1.854800\n",
       "19   2016-01-01  2.160858       222.0  1.896019\n",
       "20   2016-01-01  2.799459       232.0  1.934103\n",
       "21   2016-01-01  3.689259       257.0  2.177241\n",
       "22   2016-01-01  2.854178       259.0  2.199519\n",
       "23   2016-01-01  3.445617       282.0  1.963576\n",
       "24   2016-01-01  3.873723       294.0  1.889528\n",
       "25   2016-01-01  3.410298       298.0  2.038168\n",
       "26   2016-01-01  3.029109       303.0  2.126704\n",
       "27   2016-01-01 -0.280099       314.0  1.895892\n",
       "28   2016-01-01  2.621561       320.0  1.843713\n",
       "29   2016-01-01  2.830342       330.0  2.189949\n",
       "..          ...       ...         ...       ...\n",
       "469  2016-01-01  1.089389      7370.0  1.845518\n",
       "470  2016-01-01  6.693358      7373.0  1.655053\n",
       "471  2016-01-01  4.970713      7374.0  1.768615\n",
       "472  2016-01-01  1.193146      7389.0  2.035943\n",
       "473  2016-01-01  1.451519      7393.0  2.032126\n",
       "474  2016-01-01  1.978763      7394.0  1.846960\n",
       "475  2016-01-01  1.403638      7395.0  1.794460\n",
       "476  2016-01-01  2.178230      7396.0  2.296881\n",
       "477  2016-01-01  2.219952      7403.0  1.836180\n",
       "478  2016-01-01  1.722299      7410.0  2.327132\n",
       "479  2016-01-01  2.699415      7412.0  2.248362\n",
       "480  2016-01-01  3.212402      7419.0  1.775764\n",
       "481  2016-01-01  4.081976      7420.0  1.871855\n",
       "482  2016-01-01  1.280326      7424.0  1.727813\n",
       "483  2016-01-01  4.453304      7427.0  1.922315\n",
       "484  2016-01-01  1.692605      7428.0  1.917260\n",
       "485  2016-01-01  2.972521      7431.0  1.909445\n",
       "486  2016-01-01  4.363104      7432.0  1.943679\n",
       "487  2016-01-01  5.021493     13670.0  1.789755\n",
       "488  2016-01-01  2.988470     13674.0  2.155038\n",
       "489  2016-01-01  3.241753     13675.0  1.928294\n",
       "490  2016-01-01  4.483021     13696.0  1.798250\n",
       "491  2016-01-01  4.071138     13700.0  1.847649\n",
       "492  2016-01-01  2.167720     13710.0  1.782137\n",
       "493  2016-01-01  2.948429     13711.0  1.796199\n",
       "494  2016-01-01  2.968321     13713.0  1.936687\n",
       "495  2016-01-01  4.128452     13777.0  1.982405\n",
       "496  2016-01-01  4.884018     15000.0  1.821928\n",
       "497  2016-01-01  2.303123     15207.0  2.124604\n",
       "498  2016-01-01  2.519523     15444.0  2.059423\n",
       "\n",
       "[499 rows x 4 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df.to_csv('csv_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3055654671411516"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the mean prediction CRPS is\n",
    "np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./results/EMOS_network_theano.npy', valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard EMOS global score (in `standard_postprocessing/emos_global.R`) is 1.0654. So we are doing a little better even, but this might just be chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing for 2016 - benchmark for later models\n",
    "\n",
    "Now the same process but only for the year of 2016. We can use this as a benchmark for later models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3285, 3650)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_idx_start = return_date_idx(dates, 2016, 1, 1)\n",
    "date_idx_stop = return_date_idx(dates, 2016, 12, 31)\n",
    "date_idx_start, date_idx_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "Time: 23.99 s\n"
     ]
    }
   ],
   "source": [
    "train_crps_list, valid_crps_list = loop_over_days(\n",
    "    model_theano,\n",
    "    tobs_full, \n",
    "    tfc_full, \n",
    "    date_idx_start, date_idx_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=lr,\n",
    "    model_type='EMOS_Network_theano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98131095989952555, 0.99854569302246976)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_crps_list), np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the global EMOS benchmark for 2016 is around 1.00. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation\n",
    "\n",
    "Now let's build the same model in keras. This will provide a good starting point to expand the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the keras modules\n",
    "# Note that the cost function only works with the theano backend\n",
    "import keras\n",
    "from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build the model with Keras' functional API\n",
    "# This is quite a bit easier and shorter than in theano \n",
    "def build_EMOS_Network_keras():\n",
    "    mean_in = Input(shape=(1,))\n",
    "    std_in = Input(shape=(1,))\n",
    "    mean_out = Dense(1, activation='linear')(mean_in)\n",
    "    std_out = Dense(1, activation='linear')(std_in)\n",
    "    x = keras.layers.concatenate([mean_out, std_out], axis=1)\n",
    "    return Model(inputs=[mean_in, std_in], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_keras = build_EMOS_Network_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             2           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             2           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2)             0           dense_1[0][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model with SGD optimizer and our custom loss function\n",
    "opt = SGD(lr=0.1)  \n",
    "model_keras.compile(optimizer=opt, loss=crps_cost_function, \n",
    "                    metrics=[crps_cost_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for one day\n",
    "\n",
    "In keras the early stopping algorithm works slightly differently. It stops training once the training loss hasn't decreased by an amount delta in a certain number of steps (patience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This way we have the gradient descent on the whole training set just as in theano\n",
    "batch_size = tfc_mean_train.shape[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12619 samples, validate on 503 samples\n",
      "Epoch 1/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.3206 - crps_cost_function: 3.3206 - val_loss: 1.6199 - val_crps_cost_function: 1.6199\n",
      "Epoch 2/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.2698 - crps_cost_function: 3.2698 - val_loss: 1.6149 - val_crps_cost_function: 1.6149\n",
      "Epoch 3/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.2210 - crps_cost_function: 3.2210 - val_loss: 1.6103 - val_crps_cost_function: 1.6103\n",
      "Epoch 4/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.1739 - crps_cost_function: 3.1739 - val_loss: 1.6058 - val_crps_cost_function: 1.6058\n",
      "Epoch 5/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.1285 - crps_cost_function: 3.1285 - val_loss: 1.6015 - val_crps_cost_function: 1.6015\n",
      "Epoch 6/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.0847 - crps_cost_function: 3.0847 - val_loss: 1.5973 - val_crps_cost_function: 1.5973\n",
      "Epoch 7/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.0422 - crps_cost_function: 3.0422 - val_loss: 1.5931 - val_crps_cost_function: 1.5931\n",
      "Epoch 8/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.0010 - crps_cost_function: 3.0010 - val_loss: 1.5888 - val_crps_cost_function: 1.5888\n",
      "Epoch 9/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.9610 - crps_cost_function: 2.9610 - val_loss: 1.5845 - val_crps_cost_function: 1.5845\n",
      "Epoch 10/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.9221 - crps_cost_function: 2.9221 - val_loss: 1.5801 - val_crps_cost_function: 1.5801\n",
      "Epoch 11/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.8842 - crps_cost_function: 2.8842 - val_loss: 1.5756 - val_crps_cost_function: 1.5756\n",
      "Epoch 12/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.8473 - crps_cost_function: 2.8473 - val_loss: 1.5710 - val_crps_cost_function: 1.5710\n",
      "Epoch 13/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.8113 - crps_cost_function: 2.8113 - val_loss: 1.5663 - val_crps_cost_function: 1.5663\n",
      "Epoch 14/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.7762 - crps_cost_function: 2.7762 - val_loss: 1.5614 - val_crps_cost_function: 1.5614\n",
      "Epoch 15/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.7418 - crps_cost_function: 2.7418 - val_loss: 1.5563 - val_crps_cost_function: 1.5563\n",
      "Epoch 16/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.7082 - crps_cost_function: 2.7082 - val_loss: 1.5511 - val_crps_cost_function: 1.5511\n",
      "Epoch 17/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.6752 - crps_cost_function: 2.6752 - val_loss: 1.5457 - val_crps_cost_function: 1.5457\n",
      "Epoch 18/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.6430 - crps_cost_function: 2.6430 - val_loss: 1.5401 - val_crps_cost_function: 1.5401\n",
      "Epoch 19/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.6114 - crps_cost_function: 2.6114 - val_loss: 1.5344 - val_crps_cost_function: 1.5344\n",
      "Epoch 20/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.5803 - crps_cost_function: 2.5803 - val_loss: 1.5285 - val_crps_cost_function: 1.5285\n",
      "Epoch 21/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.5499 - crps_cost_function: 2.5499 - val_loss: 1.5224 - val_crps_cost_function: 1.5224\n",
      "Epoch 22/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.5200 - crps_cost_function: 2.5200 - val_loss: 1.5161 - val_crps_cost_function: 1.5161\n",
      "Epoch 23/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.4906 - crps_cost_function: 2.4906 - val_loss: 1.5096 - val_crps_cost_function: 1.5096\n",
      "Epoch 24/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.4617 - crps_cost_function: 2.4617 - val_loss: 1.5030 - val_crps_cost_function: 1.5030\n",
      "Epoch 25/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.4333 - crps_cost_function: 2.4333 - val_loss: 1.4962 - val_crps_cost_function: 1.4962\n",
      "Epoch 26/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.4054 - crps_cost_function: 2.4054 - val_loss: 1.4892 - val_crps_cost_function: 1.4892\n",
      "Epoch 27/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.3779 - crps_cost_function: 2.3779 - val_loss: 1.4820 - val_crps_cost_function: 1.4820\n",
      "Epoch 28/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.3509 - crps_cost_function: 2.3509 - val_loss: 1.4747 - val_crps_cost_function: 1.4747\n",
      "Epoch 29/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.3242 - crps_cost_function: 2.3242 - val_loss: 1.4673 - val_crps_cost_function: 1.4673\n",
      "Epoch 30/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.2980 - crps_cost_function: 2.2980 - val_loss: 1.4596 - val_crps_cost_function: 1.4596\n",
      "Epoch 31/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.2721 - crps_cost_function: 2.2721 - val_loss: 1.4519 - val_crps_cost_function: 1.4519\n",
      "Epoch 32/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.2466 - crps_cost_function: 2.2466 - val_loss: 1.4440 - val_crps_cost_function: 1.4440\n",
      "Epoch 33/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.2215 - crps_cost_function: 2.2215 - val_loss: 1.4359 - val_crps_cost_function: 1.4359\n",
      "Epoch 34/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1968 - crps_cost_function: 2.1968 - val_loss: 1.4277 - val_crps_cost_function: 1.4277\n",
      "Epoch 35/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1724 - crps_cost_function: 2.1724 - val_loss: 1.4194 - val_crps_cost_function: 1.4194\n",
      "Epoch 36/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1483 - crps_cost_function: 2.1483 - val_loss: 1.4109 - val_crps_cost_function: 1.4109\n",
      "Epoch 37/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1246 - crps_cost_function: 2.1246 - val_loss: 1.4024 - val_crps_cost_function: 1.4024\n",
      "Epoch 38/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1012 - crps_cost_function: 2.1012 - val_loss: 1.3937 - val_crps_cost_function: 1.3937\n",
      "Epoch 39/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.0781 - crps_cost_function: 2.0781 - val_loss: 1.3849 - val_crps_cost_function: 1.3849\n",
      "Epoch 40/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.0554 - crps_cost_function: 2.0554 - val_loss: 1.3761 - val_crps_cost_function: 1.3761\n",
      "Epoch 41/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.0330 - crps_cost_function: 2.0330 - val_loss: 1.3671 - val_crps_cost_function: 1.3671\n",
      "Epoch 42/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.0108 - crps_cost_function: 2.0108 - val_loss: 1.3580 - val_crps_cost_function: 1.3580\n",
      "Epoch 43/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9890 - crps_cost_function: 1.9890 - val_loss: 1.3489 - val_crps_cost_function: 1.3489\n",
      "Epoch 44/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9675 - crps_cost_function: 1.9675 - val_loss: 1.3397 - val_crps_cost_function: 1.3397\n",
      "Epoch 45/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9463 - crps_cost_function: 1.9463 - val_loss: 1.3304 - val_crps_cost_function: 1.3304\n",
      "Epoch 46/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9254 - crps_cost_function: 1.9254 - val_loss: 1.3211 - val_crps_cost_function: 1.3211\n",
      "Epoch 47/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9048 - crps_cost_function: 1.9048 - val_loss: 1.3117 - val_crps_cost_function: 1.3117\n",
      "Epoch 48/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8845 - crps_cost_function: 1.8845 - val_loss: 1.3022 - val_crps_cost_function: 1.3022\n",
      "Epoch 49/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8645 - crps_cost_function: 1.8645 - val_loss: 1.2927 - val_crps_cost_function: 1.2927\n",
      "Epoch 50/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8448 - crps_cost_function: 1.8448 - val_loss: 1.2832 - val_crps_cost_function: 1.2832\n",
      "Epoch 51/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8254 - crps_cost_function: 1.8254 - val_loss: 1.2736 - val_crps_cost_function: 1.2736\n",
      "Epoch 52/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8062 - crps_cost_function: 1.8062 - val_loss: 1.2641 - val_crps_cost_function: 1.2641\n",
      "Epoch 53/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7874 - crps_cost_function: 1.7874 - val_loss: 1.2545 - val_crps_cost_function: 1.2545\n",
      "Epoch 54/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7688 - crps_cost_function: 1.7688 - val_loss: 1.2449 - val_crps_cost_function: 1.2449\n",
      "Epoch 55/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7506 - crps_cost_function: 1.7506 - val_loss: 1.2353 - val_crps_cost_function: 1.2353\n",
      "Epoch 56/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7326 - crps_cost_function: 1.7326 - val_loss: 1.2257 - val_crps_cost_function: 1.2257\n",
      "Epoch 57/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7150 - crps_cost_function: 1.7150 - val_loss: 1.2161 - val_crps_cost_function: 1.2161\n",
      "Epoch 58/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6976 - crps_cost_function: 1.6976 - val_loss: 1.2065 - val_crps_cost_function: 1.2065\n",
      "Epoch 59/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6806 - crps_cost_function: 1.6806 - val_loss: 1.1970 - val_crps_cost_function: 1.1970\n",
      "Epoch 60/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6638 - crps_cost_function: 1.6638 - val_loss: 1.1875 - val_crps_cost_function: 1.1875\n",
      "Epoch 61/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6473 - crps_cost_function: 1.6473 - val_loss: 1.1780 - val_crps_cost_function: 1.1780\n",
      "Epoch 62/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6312 - crps_cost_function: 1.6312 - val_loss: 1.1686 - val_crps_cost_function: 1.1686\n",
      "Epoch 63/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6153 - crps_cost_function: 1.6153 - val_loss: 1.1592 - val_crps_cost_function: 1.1592\n",
      "Epoch 64/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5997 - crps_cost_function: 1.5997 - val_loss: 1.1499 - val_crps_cost_function: 1.1499\n",
      "Epoch 65/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5845 - crps_cost_function: 1.5845 - val_loss: 1.1407 - val_crps_cost_function: 1.1407\n",
      "Epoch 66/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5695 - crps_cost_function: 1.5695 - val_loss: 1.1315 - val_crps_cost_function: 1.1315\n",
      "Epoch 67/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5548 - crps_cost_function: 1.5548 - val_loss: 1.1224 - val_crps_cost_function: 1.1224\n",
      "Epoch 68/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5405 - crps_cost_function: 1.5405 - val_loss: 1.1134 - val_crps_cost_function: 1.1134\n",
      "Epoch 69/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5265 - crps_cost_function: 1.5265 - val_loss: 1.1045 - val_crps_cost_function: 1.1045\n",
      "Epoch 70/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5127 - crps_cost_function: 1.5127 - val_loss: 1.0956 - val_crps_cost_function: 1.0956\n",
      "Epoch 71/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4993 - crps_cost_function: 1.4993 - val_loss: 1.0869 - val_crps_cost_function: 1.0869\n",
      "Epoch 72/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4862 - crps_cost_function: 1.4862 - val_loss: 1.0783 - val_crps_cost_function: 1.0783\n",
      "Epoch 73/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4734 - crps_cost_function: 1.4734 - val_loss: 1.0698 - val_crps_cost_function: 1.0698\n",
      "Epoch 74/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4609 - crps_cost_function: 1.4609 - val_loss: 1.0614 - val_crps_cost_function: 1.0614\n",
      "Epoch 75/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4487 - crps_cost_function: 1.4487 - val_loss: 1.0532 - val_crps_cost_function: 1.0532\n",
      "Epoch 76/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4368 - crps_cost_function: 1.4368 - val_loss: 1.0450 - val_crps_cost_function: 1.0450\n",
      "Epoch 77/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4253 - crps_cost_function: 1.4253 - val_loss: 1.0370 - val_crps_cost_function: 1.0370\n",
      "Epoch 78/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4140 - crps_cost_function: 1.4140 - val_loss: 1.0292 - val_crps_cost_function: 1.0292\n",
      "Epoch 79/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4031 - crps_cost_function: 1.4031 - val_loss: 1.0215 - val_crps_cost_function: 1.0215\n",
      "Epoch 80/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3925 - crps_cost_function: 1.3925 - val_loss: 1.0139 - val_crps_cost_function: 1.0139\n",
      "Epoch 81/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3821 - crps_cost_function: 1.3821 - val_loss: 1.0065 - val_crps_cost_function: 1.0065\n",
      "Epoch 82/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3721 - crps_cost_function: 1.3721 - val_loss: 0.9992 - val_crps_cost_function: 0.9992\n",
      "Epoch 83/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3624 - crps_cost_function: 1.3624 - val_loss: 0.9921 - val_crps_cost_function: 0.9921\n",
      "Epoch 84/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3529 - crps_cost_function: 1.3529 - val_loss: 0.9851 - val_crps_cost_function: 0.9851\n",
      "Epoch 85/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3438 - crps_cost_function: 1.3438 - val_loss: 0.9784 - val_crps_cost_function: 0.9784\n",
      "Epoch 86/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3350 - crps_cost_function: 1.3350 - val_loss: 0.9717 - val_crps_cost_function: 0.9717\n",
      "Epoch 87/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3264 - crps_cost_function: 1.3264 - val_loss: 0.9653 - val_crps_cost_function: 0.9653\n",
      "Epoch 88/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3182 - crps_cost_function: 1.3182 - val_loss: 0.9590 - val_crps_cost_function: 0.9590\n",
      "Epoch 89/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3102 - crps_cost_function: 1.3102 - val_loss: 0.9528 - val_crps_cost_function: 0.9528\n",
      "Epoch 90/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3025 - crps_cost_function: 1.3025 - val_loss: 0.9469 - val_crps_cost_function: 0.9469\n",
      "Epoch 91/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2951 - crps_cost_function: 1.2951 - val_loss: 0.9411 - val_crps_cost_function: 0.9411\n",
      "Epoch 92/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2879 - crps_cost_function: 1.2879 - val_loss: 0.9354 - val_crps_cost_function: 0.9354\n",
      "Epoch 93/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2810 - crps_cost_function: 1.2810 - val_loss: 0.9300 - val_crps_cost_function: 0.9300\n",
      "Epoch 94/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2744 - crps_cost_function: 1.2744 - val_loss: 0.9247 - val_crps_cost_function: 0.9247\n",
      "Epoch 95/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2680 - crps_cost_function: 1.2680 - val_loss: 0.9196 - val_crps_cost_function: 0.9196\n",
      "Epoch 96/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2619 - crps_cost_function: 1.2619 - val_loss: 0.9146 - val_crps_cost_function: 0.9146\n",
      "Epoch 97/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2560 - crps_cost_function: 1.2560 - val_loss: 0.9098 - val_crps_cost_function: 0.9098\n",
      "Epoch 98/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2504 - crps_cost_function: 1.2504 - val_loss: 0.9052 - val_crps_cost_function: 0.9052\n",
      "Epoch 99/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2450 - crps_cost_function: 1.2450 - val_loss: 0.9007 - val_crps_cost_function: 0.9007\n",
      "Epoch 100/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2398 - crps_cost_function: 1.2398 - val_loss: 0.8964 - val_crps_cost_function: 0.8964\n",
      "Epoch 101/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2349 - crps_cost_function: 1.2349 - val_loss: 0.8922 - val_crps_cost_function: 0.8922\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12619/12619 [==============================] - 0s - loss: 1.2301 - crps_cost_function: 1.2301 - val_loss: 0.8882 - val_crps_cost_function: 0.8882\n",
      "Epoch 103/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2256 - crps_cost_function: 1.2256 - val_loss: 0.8844 - val_crps_cost_function: 0.8844\n",
      "Epoch 104/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2212 - crps_cost_function: 1.2212 - val_loss: 0.8806 - val_crps_cost_function: 0.8806\n",
      "Epoch 105/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2171 - crps_cost_function: 1.2171 - val_loss: 0.8771 - val_crps_cost_function: 0.8771\n",
      "Epoch 106/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2131 - crps_cost_function: 1.2131 - val_loss: 0.8736 - val_crps_cost_function: 0.8736\n",
      "Epoch 107/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2093 - crps_cost_function: 1.2093 - val_loss: 0.8704 - val_crps_cost_function: 0.8704\n",
      "Epoch 108/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2057 - crps_cost_function: 1.2057 - val_loss: 0.8672 - val_crps_cost_function: 0.8672\n",
      "Epoch 109/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2023 - crps_cost_function: 1.2023 - val_loss: 0.8642 - val_crps_cost_function: 0.8642\n",
      "Epoch 110/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1990 - crps_cost_function: 1.1990 - val_loss: 0.8613 - val_crps_cost_function: 0.8613\n",
      "Epoch 111/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1959 - crps_cost_function: 1.1959 - val_loss: 0.8585 - val_crps_cost_function: 0.8585\n",
      "Epoch 112/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1929 - crps_cost_function: 1.1929 - val_loss: 0.8558 - val_crps_cost_function: 0.8558\n",
      "Epoch 113/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1901 - crps_cost_function: 1.1901 - val_loss: 0.8533 - val_crps_cost_function: 0.8533\n",
      "Epoch 114/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1874 - crps_cost_function: 1.1874 - val_loss: 0.8509 - val_crps_cost_function: 0.8509\n",
      "Epoch 115/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1849 - crps_cost_function: 1.1849 - val_loss: 0.8485 - val_crps_cost_function: 0.8485\n",
      "Epoch 116/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1824 - crps_cost_function: 1.1824 - val_loss: 0.8463 - val_crps_cost_function: 0.8463\n",
      "Epoch 117/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1801 - crps_cost_function: 1.1801 - val_loss: 0.8442 - val_crps_cost_function: 0.8442\n",
      "Epoch 118/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1779 - crps_cost_function: 1.1779 - val_loss: 0.8422 - val_crps_cost_function: 0.8422\n",
      "Epoch 119/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1758 - crps_cost_function: 1.1758 - val_loss: 0.8402 - val_crps_cost_function: 0.8402\n",
      "Epoch 120/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1739 - crps_cost_function: 1.1739 - val_loss: 0.8384 - val_crps_cost_function: 0.8384\n",
      "Epoch 121/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1720 - crps_cost_function: 1.1720 - val_loss: 0.8366 - val_crps_cost_function: 0.8366\n",
      "Epoch 122/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1702 - crps_cost_function: 1.1702 - val_loss: 0.8349 - val_crps_cost_function: 0.8349\n",
      "Epoch 123/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1685 - crps_cost_function: 1.1685 - val_loss: 0.8333 - val_crps_cost_function: 0.8333\n",
      "Epoch 124/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1669 - crps_cost_function: 1.1669 - val_loss: 0.8318 - val_crps_cost_function: 0.8318\n",
      "Epoch 125/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1654 - crps_cost_function: 1.1654 - val_loss: 0.8304 - val_crps_cost_function: 0.8304\n",
      "Epoch 126/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1639 - crps_cost_function: 1.1639 - val_loss: 0.8290 - val_crps_cost_function: 0.8290\n",
      "Epoch 127/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1626 - crps_cost_function: 1.1626 - val_loss: 0.8277 - val_crps_cost_function: 0.8277\n",
      "Epoch 128/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1613 - crps_cost_function: 1.1613 - val_loss: 0.8264 - val_crps_cost_function: 0.8264\n",
      "Epoch 129/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1601 - crps_cost_function: 1.1601 - val_loss: 0.8252 - val_crps_cost_function: 0.8252\n",
      "Epoch 130/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1589 - crps_cost_function: 1.1589 - val_loss: 0.8241 - val_crps_cost_function: 0.8241\n",
      "Epoch 131/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1578 - crps_cost_function: 1.1578 - val_loss: 0.8230 - val_crps_cost_function: 0.8230\n",
      "Epoch 132/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1567 - crps_cost_function: 1.1567 - val_loss: 0.8220 - val_crps_cost_function: 0.8220\n",
      "Epoch 133/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1557 - crps_cost_function: 1.1557 - val_loss: 0.8210 - val_crps_cost_function: 0.8210\n",
      "Epoch 134/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1548 - crps_cost_function: 1.1548 - val_loss: 0.8201 - val_crps_cost_function: 0.8201\n",
      "Epoch 135/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1539 - crps_cost_function: 1.1539 - val_loss: 0.8192 - val_crps_cost_function: 0.8192\n",
      "Epoch 136/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1531 - crps_cost_function: 1.1531 - val_loss: 0.8183 - val_crps_cost_function: 0.8183\n",
      "Epoch 137/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1523 - crps_cost_function: 1.1523 - val_loss: 0.8175 - val_crps_cost_function: 0.8175\n",
      "Epoch 138/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1515 - crps_cost_function: 1.1515 - val_loss: 0.8168 - val_crps_cost_function: 0.8168\n",
      "Epoch 139/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1508 - crps_cost_function: 1.1508 - val_loss: 0.8161 - val_crps_cost_function: 0.8161\n",
      "Epoch 140/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1501 - crps_cost_function: 1.1501 - val_loss: 0.8154 - val_crps_cost_function: 0.8154\n",
      "Epoch 141/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1495 - crps_cost_function: 1.1495 - val_loss: 0.8147 - val_crps_cost_function: 0.8147\n",
      "Epoch 142/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1488 - crps_cost_function: 1.1488 - val_loss: 0.8141 - val_crps_cost_function: 0.8141\n",
      "Epoch 143/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1483 - crps_cost_function: 1.1483 - val_loss: 0.8135 - val_crps_cost_function: 0.8135\n",
      "Epoch 144/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1477 - crps_cost_function: 1.1477 - val_loss: 0.8129 - val_crps_cost_function: 0.8129\n",
      "Epoch 145/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1472 - crps_cost_function: 1.1472 - val_loss: 0.8124 - val_crps_cost_function: 0.8124\n",
      "Epoch 146/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1467 - crps_cost_function: 1.1467 - val_loss: 0.8118 - val_crps_cost_function: 0.8118\n",
      "Epoch 147/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1462 - crps_cost_function: 1.1462 - val_loss: 0.8113 - val_crps_cost_function: 0.8113\n",
      "Epoch 148/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1458 - crps_cost_function: 1.1458 - val_loss: 0.8109 - val_crps_cost_function: 0.8109\n",
      "Epoch 149/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1453 - crps_cost_function: 1.1453 - val_loss: 0.8104 - val_crps_cost_function: 0.8104\n",
      "Epoch 150/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1449 - crps_cost_function: 1.1449 - val_loss: 0.8100 - val_crps_cost_function: 0.8100\n",
      "Epoch 151/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1445 - crps_cost_function: 1.1445 - val_loss: 0.8096 - val_crps_cost_function: 0.8096\n",
      "Epoch 152/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1442 - crps_cost_function: 1.1442 - val_loss: 0.8092 - val_crps_cost_function: 0.8092\n",
      "Epoch 153/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1438 - crps_cost_function: 1.1438 - val_loss: 0.8088 - val_crps_cost_function: 0.8088\n",
      "Epoch 154/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1435 - crps_cost_function: 1.1435 - val_loss: 0.8085 - val_crps_cost_function: 0.8085\n",
      "Epoch 155/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1432 - crps_cost_function: 1.1432 - val_loss: 0.8081 - val_crps_cost_function: 0.8081\n",
      "Epoch 156/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1429 - crps_cost_function: 1.1429 - val_loss: 0.8078 - val_crps_cost_function: 0.8078\n",
      "Epoch 157/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1426 - crps_cost_function: 1.1426 - val_loss: 0.8075 - val_crps_cost_function: 0.8075\n",
      "Epoch 158/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1423 - crps_cost_function: 1.1423 - val_loss: 0.8072 - val_crps_cost_function: 0.8072\n",
      "Epoch 159/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1421 - crps_cost_function: 1.1421 - val_loss: 0.8069 - val_crps_cost_function: 0.8069\n",
      "Epoch 160/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1418 - crps_cost_function: 1.1418 - val_loss: 0.8066 - val_crps_cost_function: 0.8066\n",
      "Epoch 161/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1416 - crps_cost_function: 1.1416 - val_loss: 0.8063 - val_crps_cost_function: 0.8063\n",
      "Epoch 162/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1414 - crps_cost_function: 1.1414 - val_loss: 0.8061 - val_crps_cost_function: 0.8061\n",
      "Epoch 163/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1411 - crps_cost_function: 1.1411 - val_loss: 0.8058 - val_crps_cost_function: 0.8058\n",
      "Epoch 164/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1409 - crps_cost_function: 1.1409 - val_loss: 0.8056 - val_crps_cost_function: 0.8056\n",
      "Epoch 165/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1407 - crps_cost_function: 1.1407 - val_loss: 0.8054 - val_crps_cost_function: 0.8054\n",
      "Epoch 166/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1406 - crps_cost_function: 1.1406 - val_loss: 0.8052 - val_crps_cost_function: 0.8052\n",
      "Epoch 167/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1404 - crps_cost_function: 1.1404 - val_loss: 0.8050 - val_crps_cost_function: 0.8050\n",
      "Epoch 168/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1402 - crps_cost_function: 1.1402 - val_loss: 0.8048 - val_crps_cost_function: 0.8048\n",
      "Epoch 169/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1400 - crps_cost_function: 1.1400 - val_loss: 0.8046 - val_crps_cost_function: 0.8046\n",
      "Epoch 170/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1399 - crps_cost_function: 1.1399 - val_loss: 0.8044 - val_crps_cost_function: 0.8044\n",
      "Epoch 171/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1397 - crps_cost_function: 1.1397 - val_loss: 0.8042 - val_crps_cost_function: 0.8042\n",
      "Epoch 172/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1396 - crps_cost_function: 1.1396 - val_loss: 0.8040 - val_crps_cost_function: 0.8040\n",
      "Epoch 173/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1394 - crps_cost_function: 1.1394 - val_loss: 0.8038 - val_crps_cost_function: 0.8038\n",
      "Epoch 174/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1393 - crps_cost_function: 1.1393 - val_loss: 0.8037 - val_crps_cost_function: 0.8037\n",
      "Epoch 175/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1392 - crps_cost_function: 1.1392 - val_loss: 0.8035 - val_crps_cost_function: 0.8035\n",
      "Epoch 176/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1391 - crps_cost_function: 1.1391 - val_loss: 0.8033 - val_crps_cost_function: 0.8033\n",
      "Epoch 177/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1389 - crps_cost_function: 1.1389 - val_loss: 0.8032 - val_crps_cost_function: 0.8032\n",
      "Epoch 178/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1388 - crps_cost_function: 1.1388 - val_loss: 0.8030 - val_crps_cost_function: 0.8030\n",
      "Epoch 179/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1387 - crps_cost_function: 1.1387 - val_loss: 0.8029 - val_crps_cost_function: 0.8029\n",
      "Epoch 180/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1386 - crps_cost_function: 1.1386 - val_loss: 0.8028 - val_crps_cost_function: 0.8028\n",
      "Epoch 181/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1385 - crps_cost_function: 1.1385 - val_loss: 0.8026 - val_crps_cost_function: 0.8026\n",
      "Epoch 182/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1384 - crps_cost_function: 1.1384 - val_loss: 0.8025 - val_crps_cost_function: 0.8025\n",
      "Epoch 183/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1383 - crps_cost_function: 1.1383 - val_loss: 0.8024 - val_crps_cost_function: 0.8024\n",
      "Epoch 184/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1382 - crps_cost_function: 1.1382 - val_loss: 0.8022 - val_crps_cost_function: 0.8022\n",
      "Epoch 185/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1381 - crps_cost_function: 1.1381 - val_loss: 0.8021 - val_crps_cost_function: 0.8021\n",
      "Epoch 186/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1380 - crps_cost_function: 1.1380 - val_loss: 0.8020 - val_crps_cost_function: 0.8020\n",
      "Epoch 187/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1379 - crps_cost_function: 1.1379 - val_loss: 0.8019 - val_crps_cost_function: 0.8019\n",
      "Epoch 188/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1378 - crps_cost_function: 1.1378 - val_loss: 0.8018 - val_crps_cost_function: 0.8018\n",
      "Epoch 189/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1378 - crps_cost_function: 1.1378 - val_loss: 0.8016 - val_crps_cost_function: 0.8016\n",
      "Epoch 190/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1377 - crps_cost_function: 1.1377 - val_loss: 0.8015 - val_crps_cost_function: 0.8015\n",
      "Epoch 191/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1376 - crps_cost_function: 1.1376 - val_loss: 0.8014 - val_crps_cost_function: 0.8014\n",
      "Epoch 192/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1375 - crps_cost_function: 1.1375 - val_loss: 0.8013 - val_crps_cost_function: 0.8013\n",
      "Epoch 193/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1374 - crps_cost_function: 1.1374 - val_loss: 0.8012 - val_crps_cost_function: 0.8012\n",
      "Epoch 194/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1374 - crps_cost_function: 1.1374 - val_loss: 0.8011 - val_crps_cost_function: 0.8011\n",
      "Epoch 195/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1373 - crps_cost_function: 1.1373 - val_loss: 0.8010 - val_crps_cost_function: 0.8010\n",
      "Epoch 196/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1372 - crps_cost_function: 1.1372 - val_loss: 0.8009 - val_crps_cost_function: 0.8009\n",
      "Epoch 197/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1372 - crps_cost_function: 1.1372 - val_loss: 0.8008 - val_crps_cost_function: 0.8008\n",
      "Epoch 198/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1371 - crps_cost_function: 1.1371 - val_loss: 0.8007 - val_crps_cost_function: 0.8007\n",
      "Epoch 199/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1370 - crps_cost_function: 1.1370 - val_loss: 0.8006 - val_crps_cost_function: 0.8006\n",
      "Epoch 200/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1370 - crps_cost_function: 1.1370 - val_loss: 0.8005 - val_crps_cost_function: 0.8005\n",
      "Epoch 201/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1369 - crps_cost_function: 1.1369 - val_loss: 0.8004 - val_crps_cost_function: 0.8004\n",
      "Epoch 202/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12619/12619 [==============================] - 0s - loss: 1.1369 - crps_cost_function: 1.1369 - val_loss: 0.8003 - val_crps_cost_function: 0.8003\n",
      "Epoch 203/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1368 - crps_cost_function: 1.1368 - val_loss: 0.8002 - val_crps_cost_function: 0.8002\n",
      "Epoch 204/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1367 - crps_cost_function: 1.1367 - val_loss: 0.8001 - val_crps_cost_function: 0.8001\n",
      "Epoch 205/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1367 - crps_cost_function: 1.1367 - val_loss: 0.8000 - val_crps_cost_function: 0.8000\n",
      "Epoch 206/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1366 - crps_cost_function: 1.1366 - val_loss: 0.8000 - val_crps_cost_function: 0.8000\n",
      "Epoch 207/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1366 - crps_cost_function: 1.1366 - val_loss: 0.7999 - val_crps_cost_function: 0.7999\n",
      "Epoch 208/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1365 - crps_cost_function: 1.1365 - val_loss: 0.7998 - val_crps_cost_function: 0.7998\n",
      "Epoch 209/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1365 - crps_cost_function: 1.1365 - val_loss: 0.7997 - val_crps_cost_function: 0.7997\n",
      "Epoch 210/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1364 - crps_cost_function: 1.1364 - val_loss: 0.7996 - val_crps_cost_function: 0.7996\n",
      "Epoch 211/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1363 - crps_cost_function: 1.1363 - val_loss: 0.7995 - val_crps_cost_function: 0.7995\n",
      "Epoch 212/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1363 - crps_cost_function: 1.1363 - val_loss: 0.7994 - val_crps_cost_function: 0.7994\n",
      "Epoch 213/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1362 - crps_cost_function: 1.1362 - val_loss: 0.7994 - val_crps_cost_function: 0.7994\n",
      "Epoch 214/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1362 - crps_cost_function: 1.1362 - val_loss: 0.7993 - val_crps_cost_function: 0.7993\n",
      "Epoch 215/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1361 - crps_cost_function: 1.1361 - val_loss: 0.7992 - val_crps_cost_function: 0.7992\n",
      "Epoch 216/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1361 - crps_cost_function: 1.1361 - val_loss: 0.7991 - val_crps_cost_function: 0.7991\n",
      "Epoch 217/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1360 - crps_cost_function: 1.1360 - val_loss: 0.7990 - val_crps_cost_function: 0.7990\n",
      "Epoch 218/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1360 - crps_cost_function: 1.1360 - val_loss: 0.7990 - val_crps_cost_function: 0.7990\n",
      "Epoch 219/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1359 - crps_cost_function: 1.1359 - val_loss: 0.7989 - val_crps_cost_function: 0.7989\n",
      "Epoch 220/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1359 - crps_cost_function: 1.1359 - val_loss: 0.7988 - val_crps_cost_function: 0.7988\n",
      "Epoch 221/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1359 - crps_cost_function: 1.1359 - val_loss: 0.7987 - val_crps_cost_function: 0.7987\n",
      "Epoch 222/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1358 - crps_cost_function: 1.1358 - val_loss: 0.7986 - val_crps_cost_function: 0.7986\n",
      "Epoch 223/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1358 - crps_cost_function: 1.1358 - val_loss: 0.7986 - val_crps_cost_function: 0.7986\n",
      "Epoch 224/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1357 - crps_cost_function: 1.1357 - val_loss: 0.7985 - val_crps_cost_function: 0.7985\n",
      "Epoch 225/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1357 - crps_cost_function: 1.1357 - val_loss: 0.7984 - val_crps_cost_function: 0.7984\n",
      "Epoch 226/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1356 - crps_cost_function: 1.1356 - val_loss: 0.7983 - val_crps_cost_function: 0.7983\n",
      "Epoch 227/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1356 - crps_cost_function: 1.1356 - val_loss: 0.7983 - val_crps_cost_function: 0.7983\n",
      "Epoch 228/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1355 - crps_cost_function: 1.1355 - val_loss: 0.7982 - val_crps_cost_function: 0.7982\n",
      "Epoch 229/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1355 - crps_cost_function: 1.1355 - val_loss: 0.7981 - val_crps_cost_function: 0.7981\n",
      "Epoch 230/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1355 - crps_cost_function: 1.1355 - val_loss: 0.7980 - val_crps_cost_function: 0.7980\n",
      "Epoch 231/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1354 - crps_cost_function: 1.1354 - val_loss: 0.7980 - val_crps_cost_function: 0.7980\n",
      "Epoch 232/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1354 - crps_cost_function: 1.1354 - val_loss: 0.7979 - val_crps_cost_function: 0.7979\n",
      "Epoch 233/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1353 - crps_cost_function: 1.1353 - val_loss: 0.7978 - val_crps_cost_function: 0.7978\n",
      "Epoch 234/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1353 - crps_cost_function: 1.1353 - val_loss: 0.7978 - val_crps_cost_function: 0.7978\n",
      "Epoch 235/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1353 - crps_cost_function: 1.1353 - val_loss: 0.7977 - val_crps_cost_function: 0.7977\n",
      "Epoch 236/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1352 - crps_cost_function: 1.1352 - val_loss: 0.7976 - val_crps_cost_function: 0.7976\n",
      "Epoch 237/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1352 - crps_cost_function: 1.1352 - val_loss: 0.7975 - val_crps_cost_function: 0.7975\n",
      "Epoch 238/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1351 - crps_cost_function: 1.1351 - val_loss: 0.7975 - val_crps_cost_function: 0.7975\n",
      "Epoch 239/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1351 - crps_cost_function: 1.1351 - val_loss: 0.7974 - val_crps_cost_function: 0.7974\n",
      "Epoch 240/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1351 - crps_cost_function: 1.1351 - val_loss: 0.7973 - val_crps_cost_function: 0.7973\n",
      "Epoch 241/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1350 - crps_cost_function: 1.1350 - val_loss: 0.7973 - val_crps_cost_function: 0.7973\n",
      "Epoch 242/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1350 - crps_cost_function: 1.1350 - val_loss: 0.7972 - val_crps_cost_function: 0.7972\n",
      "Epoch 243/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1349 - crps_cost_function: 1.1349 - val_loss: 0.7971 - val_crps_cost_function: 0.7971\n",
      "Epoch 244/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1349 - crps_cost_function: 1.1349 - val_loss: 0.7971 - val_crps_cost_function: 0.7971\n",
      "Epoch 245/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1349 - crps_cost_function: 1.1349 - val_loss: 0.7970 - val_crps_cost_function: 0.7970\n",
      "Epoch 246/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1348 - crps_cost_function: 1.1348 - val_loss: 0.7969 - val_crps_cost_function: 0.7969\n",
      "Epoch 247/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1348 - crps_cost_function: 1.1348 - val_loss: 0.7969 - val_crps_cost_function: 0.7969\n",
      "Epoch 248/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1347 - crps_cost_function: 1.1347 - val_loss: 0.7968 - val_crps_cost_function: 0.7968\n",
      "Epoch 249/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1347 - crps_cost_function: 1.1347 - val_loss: 0.7967 - val_crps_cost_function: 0.7967\n",
      "Epoch 250/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1347 - crps_cost_function: 1.1347 - val_loss: 0.7967 - val_crps_cost_function: 0.7967\n",
      "Epoch 251/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1346 - crps_cost_function: 1.1346 - val_loss: 0.7966 - val_crps_cost_function: 0.7966\n",
      "Epoch 252/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1346 - crps_cost_function: 1.1346 - val_loss: 0.7965 - val_crps_cost_function: 0.7965\n",
      "Epoch 253/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1346 - crps_cost_function: 1.1346 - val_loss: 0.7965 - val_crps_cost_function: 0.7965\n",
      "Epoch 254/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1345 - crps_cost_function: 1.1345 - val_loss: 0.7964 - val_crps_cost_function: 0.7964\n",
      "Epoch 255/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1345 - crps_cost_function: 1.1345 - val_loss: 0.7963 - val_crps_cost_function: 0.7963\n",
      "Epoch 256/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1345 - crps_cost_function: 1.1345 - val_loss: 0.7963 - val_crps_cost_function: 0.7963\n",
      "Epoch 257/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1344 - crps_cost_function: 1.1344 - val_loss: 0.7962 - val_crps_cost_function: 0.7962\n",
      "Epoch 258/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1344 - crps_cost_function: 1.1344 - val_loss: 0.7961 - val_crps_cost_function: 0.7961\n",
      "Epoch 259/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1344 - crps_cost_function: 1.1344 - val_loss: 0.7961 - val_crps_cost_function: 0.7961\n",
      "Epoch 260/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1343 - crps_cost_function: 1.1343 - val_loss: 0.7960 - val_crps_cost_function: 0.7960\n",
      "Epoch 261/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1343 - crps_cost_function: 1.1343 - val_loss: 0.7959 - val_crps_cost_function: 0.7959\n",
      "Epoch 262/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1342 - crps_cost_function: 1.1342 - val_loss: 0.7959 - val_crps_cost_function: 0.7959\n",
      "Epoch 263/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1342 - crps_cost_function: 1.1342 - val_loss: 0.7958 - val_crps_cost_function: 0.7958\n",
      "Epoch 264/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1342 - crps_cost_function: 1.1342 - val_loss: 0.7958 - val_crps_cost_function: 0.7958\n",
      "Epoch 265/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1341 - crps_cost_function: 1.1341 - val_loss: 0.7957 - val_crps_cost_function: 0.7957\n",
      "Epoch 266/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1341 - crps_cost_function: 1.1341 - val_loss: 0.7956 - val_crps_cost_function: 0.7956\n",
      "Epoch 267/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1341 - crps_cost_function: 1.1341 - val_loss: 0.7956 - val_crps_cost_function: 0.7956\n",
      "Epoch 268/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1340 - crps_cost_function: 1.1340 - val_loss: 0.7955 - val_crps_cost_function: 0.7955\n"
     ]
    }
   ],
   "source": [
    "model_keras.fit([tfc_mean_train, tfc_std_train], tobs_train, epochs=steps_max, batch_size=batch_size,\n",
    "          validation_data=[[tfc_mean_test, tfc_std_test], tobs_test], verbose=0,\n",
    "          callbacks=[EarlyStopping(monitor='loss', min_delta=early_stopping_delta,\n",
    "                                  patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train and test CRPS\n",
    "(model_keras.evaluate([tfc_mean_train, tfc_std_train], tobs_train, batch_size, verbose=0), \n",
    " model_keras.evaluate([tfc_mean_test, tfc_std_test], tobs_test, batch_size, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the CRPS is slightly higher than in our theano implementation. This is most likely due to the differences in the early stopping algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing for 2016\n",
    "\n",
    "Same as above with the theano model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_idx_start = return_date_idx(dates, 2016, 1, 1)\n",
    "date_idx_stop = return_date_idx(dates, 2016, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_keras = build_EMOS_Network_keras()\n",
    "opt = SGD(lr=0.1)  \n",
    "model_keras.compile(optimizer=opt, loss=crps_cost_function, \n",
    "                    metrics=[crps_cost_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:14<00:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 74.96 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This function loops over the days.\n",
    "train_crps_list, valid_crps_list = loop_over_days(\n",
    "    DATA_DIR,\n",
    "    model_keras,\n",
    "    date_str_start, date_str_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=lr,\n",
    "    verbose=0,\n",
    "    model_type='EMOS_Network_keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the keras implementation is quite a bit slower than the pure theano version. This could be due to the overhead of calling model.fit many many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1908277614435003, 1.3074258575308064)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_crps_list), np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very similar to the theano implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2015, predict 2016\n",
    "\n",
    "Finally, we will train one single model on all of the 2015 data, and then post-process all of 2016 with this one model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfc_mean_train, tfc_std_train, tobs_train, tfc_mean_test, tfc_std_test, tobs_test = \\\n",
    "        get_train_test_data(tobs_full, tfc_full, date_idx_start, window_size=365, fclt=0, \n",
    "                            subtract_std_mean=False, test_plus=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180849,), (181718,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfc_mean_train.shape, tfc_mean_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_keras = build_EMOS_Network_keras()\n",
    "opt = Adam(lr=0.1)  # Adam is a better SGD in a nutshell\n",
    "model_keras.compile(optimizer=opt, loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 181718 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 2.3505 - val_loss: 1.0104\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0107\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0120\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0158\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0694 - val_loss: 1.0093\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0695 - val_loss: 1.0099\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0694 - val_loss: 1.0098\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0695 - val_loss: 1.0121\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0697 - val_loss: 1.0108\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0696 - val_loss: 1.0087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124a514a8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_keras.fit([tfc_mean_train, tfc_std_train], tobs_train, epochs=10, \n",
    "                batch_size=1024, \n",
    "                validation_data=[[tfc_mean_test, tfc_std_test], tobs_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get a 2016 CRPS of 1.01 compared to 1.00 for the 25 day rolling window. Surprisingly little difference. This suggests that the seasonality is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
