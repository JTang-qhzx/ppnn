{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Prepare-the-data\" data-toc-modified-id=\"Prepare-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prepare the data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Theano-Implementation\" data-toc-modified-id=\"Theano-Implementation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Theano Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Train-for-a-single-day\" data-toc-modified-id=\"Train-for-a-single-day-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Train for a single day</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Post-processing-for-the-entire-period\" data-toc-modified-id=\"Post-processing-for-the-entire-period-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Post processing for the entire period</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Post-processing-for-2016---benchmark-for-later-models\" data-toc-modified-id=\"Post-processing-for-2016---benchmark-for-later-models-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Post-processing for 2016 - benchmark for later models</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Keras-implementation\" data-toc-modified-id=\"Keras-implementation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Keras implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Predict-for-one-day\" data-toc-modified-id=\"Predict-for-one-day-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Predict for one day</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Post-processing-for-2016\" data-toc-modified-id=\"Post-processing-for-2016-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Post-processing for 2016</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Train-2015,-predict-2016\" data-toc-modified-id=\"Train-2015,-predict-2016-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Train 2015, predict 2016</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMOS Network\n",
    "\n",
    "The goal of this notebooks is to build and test a network implementation of EMOS, once in pure theano and then in keras. First we will try to replicate the results of the standard global EMOS. \n",
    "\n",
    "The reference period is always the mean CRPS for all dates and stations in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Note that the cost function only works with the theano backend for keras\n",
    "from importlib import reload\n",
    "import emos_network_theano; reload(emos_network_theano)\n",
    "from  emos_network_theano import EMOS_Network\n",
    "from crps_loss import crps_cost_function\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "DATA_DIR = '/Volumes/STICK/data/ppnn_data/'  # Mac\n",
    "# DATA_DIR = '/project/meteo/w2w/C7/ppnn_data/'   # LMU\n",
    "fn = 'data_interpolated_00UTC.nc'   # Temperature observation and forecast data\n",
    "window_size = 25   # Days in rolling window\n",
    "fclt = 48   # Forecast lead time in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "The interpolated data set contains the observations and forecasts from the 50 members for each station and each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3653, 537), (3653, 50, 537), (3653,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "tobs_full, tfc_full, dates = load_nc_data(DATA_DIR + fn)\n",
    "tobs_full.shape, tfc_full.shape, dates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert this dataset to use as the input for the EMOS networks. For this we pick a forecast date, and return as the training data all previous days within the window size previous to the start of the forecast. The test data is simply the data for the chosen forecast date. The 50 member ensemble is summarized by the mean and the standard deviation. Additionally, we remove all data where the observations are missing. Finally, the inputs are scaled. For this we subtract the mean from the mean and divide both the mean and the standard deviation by their standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's chose a random day to illustrate this\n",
    "date_idx = return_date_idx(dates, 2011, 2, 14)   # year, month, day\n",
    "date_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now get the training and test data for this day\n",
    "tfc_mean_train, tfc_std_train, tobs_train, tfc_mean_test, tfc_std_test, tobs_test = \\\n",
    "        get_train_test_data(tobs_full, tfc_full, date_idx, window_size, fclt, \n",
    "                            subtract_std_mean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12619,), (503,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfc_mean_train.shape, tfc_mean_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 1D arrays contain the data for all dates and stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano Implementation\n",
    "\n",
    "To start with we build the model in pure theano. The model is defined in a separate script `EMOS_network_theano.py`. The network uses a custom CRPS loss function which is defined in `crps_loss.py`.\n",
    "\n",
    "The EMOS_Network class is build to work in a similar way to keras models. For the fitting we are using gradient descent. Since we are using the entire dataset for each update, it is not stochastic. An early stopping algorithm is built into the fitting function. It stops training if the average training CRPS of the last 5 steps is decreasing by less than a parameter delta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for a single day\n",
    "\n",
    "To illustrate how the model work we will use the data for our example day above and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some model parameters\n",
    "lr = np.asarray(0.1, dtype='float32')   # The learning rate\n",
    "early_stopping_delta = 1e-4   # How much the CRPS must improve before stopping\n",
    "steps_max = 1000   # How many steps to fit at max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the theano model\n",
    "model_theano = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1.146521381814908), array(0.8170318748490368))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for some steps\n",
    "model_theano.fit(tfc_mean_train, tfc_std_train, tobs_train, steps_max, \n",
    "          (tfc_mean_test, tfc_std_test, tobs_test), lr=lr, \n",
    "          early_stopping_delta=early_stopping_delta)\n",
    "# Output is the training CRPS and the test CRPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing for the entire period\n",
    "\n",
    "To compare the network model with the standard EMOS we will run it from 1 January 2008 to 31 December 2016. When looping over the days we are not resetting the model weights for each day. This drastically reduces the training time with identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 3650)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get start and stop indices\n",
    "date_idx_start = return_date_idx(dates, 2008, 1, 1)\n",
    "date_idx_stop = return_date_idx(dates, 2016, 12, 31)\n",
    "date_idx_start, date_idx_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_theano = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "Time: 208.87 s\n"
     ]
    }
   ],
   "source": [
    "# This function loops over the days.\n",
    "train_crps_list, valid_crps_list = loop_over_days(\n",
    "    model_theano,\n",
    "    tobs_full, \n",
    "    tfc_full, \n",
    "    date_idx_start, date_idx_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=lr,\n",
    "    model_type='EMOS_Network_theano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0558060735203831"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the mean prediction CRPS is\n",
    "np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./results/EMOS_network_theano.npy', valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard EMOS global score (in `standard_postprocessing/emos_global.R`) is 1.0654. So we are doing a little better even, but this might just be chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing for 2016 - benchmark for later models\n",
    "\n",
    "Now the same process but only for the year of 2016. We can use this as a benchmark for later models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3285, 3650)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_idx_start = return_date_idx(dates, 2016, 1, 1)\n",
    "date_idx_stop = return_date_idx(dates, 2016, 12, 31)\n",
    "date_idx_start, date_idx_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "Time: 23.99 s\n"
     ]
    }
   ],
   "source": [
    "train_crps_list, valid_crps_list = loop_over_days(\n",
    "    model_theano,\n",
    "    tobs_full, \n",
    "    tfc_full, \n",
    "    date_idx_start, date_idx_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=lr,\n",
    "    model_type='EMOS_Network_theano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98131095989952555, 0.99854569302246976)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_crps_list), np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the global EMOS benchmark for 2016 is around 1.00. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation\n",
    "\n",
    "Now let's build the same model in keras. This will provide a good starting point to expand the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the keras modules\n",
    "# Note that the cost function only works with the theano backend\n",
    "import keras\n",
    "from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build the model with Keras' functional API\n",
    "# This is quite a bit easier and shorter than in theano \n",
    "def build_EMOS_Network_keras():\n",
    "    mean_in = Input(shape=(1,))\n",
    "    std_in = Input(shape=(1,))\n",
    "    mean_out = Dense(1, activation='linear')(mean_in)\n",
    "    std_out = Dense(1, activation='linear')(std_in)\n",
    "    x = keras.layers.concatenate([mean_out, std_out], axis=1)\n",
    "    return Model(inputs=[mean_in, std_in], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_keras = build_EMOS_Network_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             2           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             2           input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2)             0           dense_1[0][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model with SGD optimizer and our custom loss function\n",
    "opt = SGD(lr=0.1)  \n",
    "model_keras.compile(optimizer=opt, loss=crps_cost_function, \n",
    "                    metrics=[crps_cost_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for one day\n",
    "\n",
    "In keras the early stopping algorithm works slightly differently. It stops training once the training loss hasn't decreased by an amount delta in a certain number of steps (patience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This way we have the gradient descent on the whole training set just as in theano\n",
    "batch_size = tfc_mean_train.shape[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12619 samples, validate on 503 samples\n",
      "Epoch 1/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.3206 - crps_cost_function: 3.3206 - val_loss: 1.6199 - val_crps_cost_function: 1.6199\n",
      "Epoch 2/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.2698 - crps_cost_function: 3.2698 - val_loss: 1.6149 - val_crps_cost_function: 1.6149\n",
      "Epoch 3/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.2210 - crps_cost_function: 3.2210 - val_loss: 1.6103 - val_crps_cost_function: 1.6103\n",
      "Epoch 4/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.1739 - crps_cost_function: 3.1739 - val_loss: 1.6058 - val_crps_cost_function: 1.6058\n",
      "Epoch 5/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.1285 - crps_cost_function: 3.1285 - val_loss: 1.6015 - val_crps_cost_function: 1.6015\n",
      "Epoch 6/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.0847 - crps_cost_function: 3.0847 - val_loss: 1.5973 - val_crps_cost_function: 1.5973\n",
      "Epoch 7/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.0422 - crps_cost_function: 3.0422 - val_loss: 1.5931 - val_crps_cost_function: 1.5931\n",
      "Epoch 8/1000\n",
      "12619/12619 [==============================] - 0s - loss: 3.0010 - crps_cost_function: 3.0010 - val_loss: 1.5888 - val_crps_cost_function: 1.5888\n",
      "Epoch 9/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.9610 - crps_cost_function: 2.9610 - val_loss: 1.5845 - val_crps_cost_function: 1.5845\n",
      "Epoch 10/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.9221 - crps_cost_function: 2.9221 - val_loss: 1.5801 - val_crps_cost_function: 1.5801\n",
      "Epoch 11/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.8842 - crps_cost_function: 2.8842 - val_loss: 1.5756 - val_crps_cost_function: 1.5756\n",
      "Epoch 12/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.8473 - crps_cost_function: 2.8473 - val_loss: 1.5710 - val_crps_cost_function: 1.5710\n",
      "Epoch 13/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.8113 - crps_cost_function: 2.8113 - val_loss: 1.5663 - val_crps_cost_function: 1.5663\n",
      "Epoch 14/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.7762 - crps_cost_function: 2.7762 - val_loss: 1.5614 - val_crps_cost_function: 1.5614\n",
      "Epoch 15/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.7418 - crps_cost_function: 2.7418 - val_loss: 1.5563 - val_crps_cost_function: 1.5563\n",
      "Epoch 16/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.7082 - crps_cost_function: 2.7082 - val_loss: 1.5511 - val_crps_cost_function: 1.5511\n",
      "Epoch 17/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.6752 - crps_cost_function: 2.6752 - val_loss: 1.5457 - val_crps_cost_function: 1.5457\n",
      "Epoch 18/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.6430 - crps_cost_function: 2.6430 - val_loss: 1.5401 - val_crps_cost_function: 1.5401\n",
      "Epoch 19/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.6114 - crps_cost_function: 2.6114 - val_loss: 1.5344 - val_crps_cost_function: 1.5344\n",
      "Epoch 20/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.5803 - crps_cost_function: 2.5803 - val_loss: 1.5285 - val_crps_cost_function: 1.5285\n",
      "Epoch 21/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.5499 - crps_cost_function: 2.5499 - val_loss: 1.5224 - val_crps_cost_function: 1.5224\n",
      "Epoch 22/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.5200 - crps_cost_function: 2.5200 - val_loss: 1.5161 - val_crps_cost_function: 1.5161\n",
      "Epoch 23/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.4906 - crps_cost_function: 2.4906 - val_loss: 1.5096 - val_crps_cost_function: 1.5096\n",
      "Epoch 24/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.4617 - crps_cost_function: 2.4617 - val_loss: 1.5030 - val_crps_cost_function: 1.5030\n",
      "Epoch 25/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.4333 - crps_cost_function: 2.4333 - val_loss: 1.4962 - val_crps_cost_function: 1.4962\n",
      "Epoch 26/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.4054 - crps_cost_function: 2.4054 - val_loss: 1.4892 - val_crps_cost_function: 1.4892\n",
      "Epoch 27/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.3779 - crps_cost_function: 2.3779 - val_loss: 1.4820 - val_crps_cost_function: 1.4820\n",
      "Epoch 28/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.3509 - crps_cost_function: 2.3509 - val_loss: 1.4747 - val_crps_cost_function: 1.4747\n",
      "Epoch 29/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.3242 - crps_cost_function: 2.3242 - val_loss: 1.4673 - val_crps_cost_function: 1.4673\n",
      "Epoch 30/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.2980 - crps_cost_function: 2.2980 - val_loss: 1.4596 - val_crps_cost_function: 1.4596\n",
      "Epoch 31/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.2721 - crps_cost_function: 2.2721 - val_loss: 1.4519 - val_crps_cost_function: 1.4519\n",
      "Epoch 32/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.2466 - crps_cost_function: 2.2466 - val_loss: 1.4440 - val_crps_cost_function: 1.4440\n",
      "Epoch 33/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.2215 - crps_cost_function: 2.2215 - val_loss: 1.4359 - val_crps_cost_function: 1.4359\n",
      "Epoch 34/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1968 - crps_cost_function: 2.1968 - val_loss: 1.4277 - val_crps_cost_function: 1.4277\n",
      "Epoch 35/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1724 - crps_cost_function: 2.1724 - val_loss: 1.4194 - val_crps_cost_function: 1.4194\n",
      "Epoch 36/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1483 - crps_cost_function: 2.1483 - val_loss: 1.4109 - val_crps_cost_function: 1.4109\n",
      "Epoch 37/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1246 - crps_cost_function: 2.1246 - val_loss: 1.4024 - val_crps_cost_function: 1.4024\n",
      "Epoch 38/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.1012 - crps_cost_function: 2.1012 - val_loss: 1.3937 - val_crps_cost_function: 1.3937\n",
      "Epoch 39/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.0781 - crps_cost_function: 2.0781 - val_loss: 1.3849 - val_crps_cost_function: 1.3849\n",
      "Epoch 40/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.0554 - crps_cost_function: 2.0554 - val_loss: 1.3761 - val_crps_cost_function: 1.3761\n",
      "Epoch 41/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.0330 - crps_cost_function: 2.0330 - val_loss: 1.3671 - val_crps_cost_function: 1.3671\n",
      "Epoch 42/1000\n",
      "12619/12619 [==============================] - 0s - loss: 2.0108 - crps_cost_function: 2.0108 - val_loss: 1.3580 - val_crps_cost_function: 1.3580\n",
      "Epoch 43/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9890 - crps_cost_function: 1.9890 - val_loss: 1.3489 - val_crps_cost_function: 1.3489\n",
      "Epoch 44/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9675 - crps_cost_function: 1.9675 - val_loss: 1.3397 - val_crps_cost_function: 1.3397\n",
      "Epoch 45/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9463 - crps_cost_function: 1.9463 - val_loss: 1.3304 - val_crps_cost_function: 1.3304\n",
      "Epoch 46/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9254 - crps_cost_function: 1.9254 - val_loss: 1.3211 - val_crps_cost_function: 1.3211\n",
      "Epoch 47/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.9048 - crps_cost_function: 1.9048 - val_loss: 1.3117 - val_crps_cost_function: 1.3117\n",
      "Epoch 48/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8845 - crps_cost_function: 1.8845 - val_loss: 1.3022 - val_crps_cost_function: 1.3022\n",
      "Epoch 49/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8645 - crps_cost_function: 1.8645 - val_loss: 1.2927 - val_crps_cost_function: 1.2927\n",
      "Epoch 50/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8448 - crps_cost_function: 1.8448 - val_loss: 1.2832 - val_crps_cost_function: 1.2832\n",
      "Epoch 51/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8254 - crps_cost_function: 1.8254 - val_loss: 1.2736 - val_crps_cost_function: 1.2736\n",
      "Epoch 52/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.8062 - crps_cost_function: 1.8062 - val_loss: 1.2641 - val_crps_cost_function: 1.2641\n",
      "Epoch 53/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7874 - crps_cost_function: 1.7874 - val_loss: 1.2545 - val_crps_cost_function: 1.2545\n",
      "Epoch 54/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7688 - crps_cost_function: 1.7688 - val_loss: 1.2449 - val_crps_cost_function: 1.2449\n",
      "Epoch 55/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7506 - crps_cost_function: 1.7506 - val_loss: 1.2353 - val_crps_cost_function: 1.2353\n",
      "Epoch 56/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7326 - crps_cost_function: 1.7326 - val_loss: 1.2257 - val_crps_cost_function: 1.2257\n",
      "Epoch 57/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.7150 - crps_cost_function: 1.7150 - val_loss: 1.2161 - val_crps_cost_function: 1.2161\n",
      "Epoch 58/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6976 - crps_cost_function: 1.6976 - val_loss: 1.2065 - val_crps_cost_function: 1.2065\n",
      "Epoch 59/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6806 - crps_cost_function: 1.6806 - val_loss: 1.1970 - val_crps_cost_function: 1.1970\n",
      "Epoch 60/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6638 - crps_cost_function: 1.6638 - val_loss: 1.1875 - val_crps_cost_function: 1.1875\n",
      "Epoch 61/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6473 - crps_cost_function: 1.6473 - val_loss: 1.1780 - val_crps_cost_function: 1.1780\n",
      "Epoch 62/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6312 - crps_cost_function: 1.6312 - val_loss: 1.1686 - val_crps_cost_function: 1.1686\n",
      "Epoch 63/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.6153 - crps_cost_function: 1.6153 - val_loss: 1.1592 - val_crps_cost_function: 1.1592\n",
      "Epoch 64/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5997 - crps_cost_function: 1.5997 - val_loss: 1.1499 - val_crps_cost_function: 1.1499\n",
      "Epoch 65/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5845 - crps_cost_function: 1.5845 - val_loss: 1.1407 - val_crps_cost_function: 1.1407\n",
      "Epoch 66/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5695 - crps_cost_function: 1.5695 - val_loss: 1.1315 - val_crps_cost_function: 1.1315\n",
      "Epoch 67/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5548 - crps_cost_function: 1.5548 - val_loss: 1.1224 - val_crps_cost_function: 1.1224\n",
      "Epoch 68/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5405 - crps_cost_function: 1.5405 - val_loss: 1.1134 - val_crps_cost_function: 1.1134\n",
      "Epoch 69/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5265 - crps_cost_function: 1.5265 - val_loss: 1.1045 - val_crps_cost_function: 1.1045\n",
      "Epoch 70/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.5127 - crps_cost_function: 1.5127 - val_loss: 1.0956 - val_crps_cost_function: 1.0956\n",
      "Epoch 71/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4993 - crps_cost_function: 1.4993 - val_loss: 1.0869 - val_crps_cost_function: 1.0869\n",
      "Epoch 72/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4862 - crps_cost_function: 1.4862 - val_loss: 1.0783 - val_crps_cost_function: 1.0783\n",
      "Epoch 73/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4734 - crps_cost_function: 1.4734 - val_loss: 1.0698 - val_crps_cost_function: 1.0698\n",
      "Epoch 74/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4609 - crps_cost_function: 1.4609 - val_loss: 1.0614 - val_crps_cost_function: 1.0614\n",
      "Epoch 75/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4487 - crps_cost_function: 1.4487 - val_loss: 1.0532 - val_crps_cost_function: 1.0532\n",
      "Epoch 76/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4368 - crps_cost_function: 1.4368 - val_loss: 1.0450 - val_crps_cost_function: 1.0450\n",
      "Epoch 77/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4253 - crps_cost_function: 1.4253 - val_loss: 1.0370 - val_crps_cost_function: 1.0370\n",
      "Epoch 78/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4140 - crps_cost_function: 1.4140 - val_loss: 1.0292 - val_crps_cost_function: 1.0292\n",
      "Epoch 79/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.4031 - crps_cost_function: 1.4031 - val_loss: 1.0215 - val_crps_cost_function: 1.0215\n",
      "Epoch 80/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3925 - crps_cost_function: 1.3925 - val_loss: 1.0139 - val_crps_cost_function: 1.0139\n",
      "Epoch 81/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3821 - crps_cost_function: 1.3821 - val_loss: 1.0065 - val_crps_cost_function: 1.0065\n",
      "Epoch 82/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3721 - crps_cost_function: 1.3721 - val_loss: 0.9992 - val_crps_cost_function: 0.9992\n",
      "Epoch 83/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3624 - crps_cost_function: 1.3624 - val_loss: 0.9921 - val_crps_cost_function: 0.9921\n",
      "Epoch 84/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3529 - crps_cost_function: 1.3529 - val_loss: 0.9851 - val_crps_cost_function: 0.9851\n",
      "Epoch 85/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3438 - crps_cost_function: 1.3438 - val_loss: 0.9784 - val_crps_cost_function: 0.9784\n",
      "Epoch 86/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3350 - crps_cost_function: 1.3350 - val_loss: 0.9717 - val_crps_cost_function: 0.9717\n",
      "Epoch 87/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3264 - crps_cost_function: 1.3264 - val_loss: 0.9653 - val_crps_cost_function: 0.9653\n",
      "Epoch 88/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3182 - crps_cost_function: 1.3182 - val_loss: 0.9590 - val_crps_cost_function: 0.9590\n",
      "Epoch 89/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3102 - crps_cost_function: 1.3102 - val_loss: 0.9528 - val_crps_cost_function: 0.9528\n",
      "Epoch 90/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.3025 - crps_cost_function: 1.3025 - val_loss: 0.9469 - val_crps_cost_function: 0.9469\n",
      "Epoch 91/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2951 - crps_cost_function: 1.2951 - val_loss: 0.9411 - val_crps_cost_function: 0.9411\n",
      "Epoch 92/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2879 - crps_cost_function: 1.2879 - val_loss: 0.9354 - val_crps_cost_function: 0.9354\n",
      "Epoch 93/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2810 - crps_cost_function: 1.2810 - val_loss: 0.9300 - val_crps_cost_function: 0.9300\n",
      "Epoch 94/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2744 - crps_cost_function: 1.2744 - val_loss: 0.9247 - val_crps_cost_function: 0.9247\n",
      "Epoch 95/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2680 - crps_cost_function: 1.2680 - val_loss: 0.9196 - val_crps_cost_function: 0.9196\n",
      "Epoch 96/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2619 - crps_cost_function: 1.2619 - val_loss: 0.9146 - val_crps_cost_function: 0.9146\n",
      "Epoch 97/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2560 - crps_cost_function: 1.2560 - val_loss: 0.9098 - val_crps_cost_function: 0.9098\n",
      "Epoch 98/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2504 - crps_cost_function: 1.2504 - val_loss: 0.9052 - val_crps_cost_function: 0.9052\n",
      "Epoch 99/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2450 - crps_cost_function: 1.2450 - val_loss: 0.9007 - val_crps_cost_function: 0.9007\n",
      "Epoch 100/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2398 - crps_cost_function: 1.2398 - val_loss: 0.8964 - val_crps_cost_function: 0.8964\n",
      "Epoch 101/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2349 - crps_cost_function: 1.2349 - val_loss: 0.8922 - val_crps_cost_function: 0.8922\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12619/12619 [==============================] - 0s - loss: 1.2301 - crps_cost_function: 1.2301 - val_loss: 0.8882 - val_crps_cost_function: 0.8882\n",
      "Epoch 103/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2256 - crps_cost_function: 1.2256 - val_loss: 0.8844 - val_crps_cost_function: 0.8844\n",
      "Epoch 104/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2212 - crps_cost_function: 1.2212 - val_loss: 0.8806 - val_crps_cost_function: 0.8806\n",
      "Epoch 105/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2171 - crps_cost_function: 1.2171 - val_loss: 0.8771 - val_crps_cost_function: 0.8771\n",
      "Epoch 106/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2131 - crps_cost_function: 1.2131 - val_loss: 0.8736 - val_crps_cost_function: 0.8736\n",
      "Epoch 107/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2093 - crps_cost_function: 1.2093 - val_loss: 0.8704 - val_crps_cost_function: 0.8704\n",
      "Epoch 108/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2057 - crps_cost_function: 1.2057 - val_loss: 0.8672 - val_crps_cost_function: 0.8672\n",
      "Epoch 109/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.2023 - crps_cost_function: 1.2023 - val_loss: 0.8642 - val_crps_cost_function: 0.8642\n",
      "Epoch 110/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1990 - crps_cost_function: 1.1990 - val_loss: 0.8613 - val_crps_cost_function: 0.8613\n",
      "Epoch 111/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1959 - crps_cost_function: 1.1959 - val_loss: 0.8585 - val_crps_cost_function: 0.8585\n",
      "Epoch 112/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1929 - crps_cost_function: 1.1929 - val_loss: 0.8558 - val_crps_cost_function: 0.8558\n",
      "Epoch 113/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1901 - crps_cost_function: 1.1901 - val_loss: 0.8533 - val_crps_cost_function: 0.8533\n",
      "Epoch 114/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1874 - crps_cost_function: 1.1874 - val_loss: 0.8509 - val_crps_cost_function: 0.8509\n",
      "Epoch 115/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1849 - crps_cost_function: 1.1849 - val_loss: 0.8485 - val_crps_cost_function: 0.8485\n",
      "Epoch 116/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1824 - crps_cost_function: 1.1824 - val_loss: 0.8463 - val_crps_cost_function: 0.8463\n",
      "Epoch 117/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1801 - crps_cost_function: 1.1801 - val_loss: 0.8442 - val_crps_cost_function: 0.8442\n",
      "Epoch 118/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1779 - crps_cost_function: 1.1779 - val_loss: 0.8422 - val_crps_cost_function: 0.8422\n",
      "Epoch 119/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1758 - crps_cost_function: 1.1758 - val_loss: 0.8402 - val_crps_cost_function: 0.8402\n",
      "Epoch 120/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1739 - crps_cost_function: 1.1739 - val_loss: 0.8384 - val_crps_cost_function: 0.8384\n",
      "Epoch 121/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1720 - crps_cost_function: 1.1720 - val_loss: 0.8366 - val_crps_cost_function: 0.8366\n",
      "Epoch 122/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1702 - crps_cost_function: 1.1702 - val_loss: 0.8349 - val_crps_cost_function: 0.8349\n",
      "Epoch 123/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1685 - crps_cost_function: 1.1685 - val_loss: 0.8333 - val_crps_cost_function: 0.8333\n",
      "Epoch 124/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1669 - crps_cost_function: 1.1669 - val_loss: 0.8318 - val_crps_cost_function: 0.8318\n",
      "Epoch 125/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1654 - crps_cost_function: 1.1654 - val_loss: 0.8304 - val_crps_cost_function: 0.8304\n",
      "Epoch 126/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1639 - crps_cost_function: 1.1639 - val_loss: 0.8290 - val_crps_cost_function: 0.8290\n",
      "Epoch 127/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1626 - crps_cost_function: 1.1626 - val_loss: 0.8277 - val_crps_cost_function: 0.8277\n",
      "Epoch 128/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1613 - crps_cost_function: 1.1613 - val_loss: 0.8264 - val_crps_cost_function: 0.8264\n",
      "Epoch 129/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1601 - crps_cost_function: 1.1601 - val_loss: 0.8252 - val_crps_cost_function: 0.8252\n",
      "Epoch 130/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1589 - crps_cost_function: 1.1589 - val_loss: 0.8241 - val_crps_cost_function: 0.8241\n",
      "Epoch 131/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1578 - crps_cost_function: 1.1578 - val_loss: 0.8230 - val_crps_cost_function: 0.8230\n",
      "Epoch 132/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1567 - crps_cost_function: 1.1567 - val_loss: 0.8220 - val_crps_cost_function: 0.8220\n",
      "Epoch 133/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1557 - crps_cost_function: 1.1557 - val_loss: 0.8210 - val_crps_cost_function: 0.8210\n",
      "Epoch 134/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1548 - crps_cost_function: 1.1548 - val_loss: 0.8201 - val_crps_cost_function: 0.8201\n",
      "Epoch 135/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1539 - crps_cost_function: 1.1539 - val_loss: 0.8192 - val_crps_cost_function: 0.8192\n",
      "Epoch 136/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1531 - crps_cost_function: 1.1531 - val_loss: 0.8183 - val_crps_cost_function: 0.8183\n",
      "Epoch 137/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1523 - crps_cost_function: 1.1523 - val_loss: 0.8175 - val_crps_cost_function: 0.8175\n",
      "Epoch 138/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1515 - crps_cost_function: 1.1515 - val_loss: 0.8168 - val_crps_cost_function: 0.8168\n",
      "Epoch 139/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1508 - crps_cost_function: 1.1508 - val_loss: 0.8161 - val_crps_cost_function: 0.8161\n",
      "Epoch 140/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1501 - crps_cost_function: 1.1501 - val_loss: 0.8154 - val_crps_cost_function: 0.8154\n",
      "Epoch 141/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1495 - crps_cost_function: 1.1495 - val_loss: 0.8147 - val_crps_cost_function: 0.8147\n",
      "Epoch 142/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1488 - crps_cost_function: 1.1488 - val_loss: 0.8141 - val_crps_cost_function: 0.8141\n",
      "Epoch 143/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1483 - crps_cost_function: 1.1483 - val_loss: 0.8135 - val_crps_cost_function: 0.8135\n",
      "Epoch 144/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1477 - crps_cost_function: 1.1477 - val_loss: 0.8129 - val_crps_cost_function: 0.8129\n",
      "Epoch 145/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1472 - crps_cost_function: 1.1472 - val_loss: 0.8124 - val_crps_cost_function: 0.8124\n",
      "Epoch 146/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1467 - crps_cost_function: 1.1467 - val_loss: 0.8118 - val_crps_cost_function: 0.8118\n",
      "Epoch 147/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1462 - crps_cost_function: 1.1462 - val_loss: 0.8113 - val_crps_cost_function: 0.8113\n",
      "Epoch 148/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1458 - crps_cost_function: 1.1458 - val_loss: 0.8109 - val_crps_cost_function: 0.8109\n",
      "Epoch 149/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1453 - crps_cost_function: 1.1453 - val_loss: 0.8104 - val_crps_cost_function: 0.8104\n",
      "Epoch 150/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1449 - crps_cost_function: 1.1449 - val_loss: 0.8100 - val_crps_cost_function: 0.8100\n",
      "Epoch 151/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1445 - crps_cost_function: 1.1445 - val_loss: 0.8096 - val_crps_cost_function: 0.8096\n",
      "Epoch 152/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1442 - crps_cost_function: 1.1442 - val_loss: 0.8092 - val_crps_cost_function: 0.8092\n",
      "Epoch 153/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1438 - crps_cost_function: 1.1438 - val_loss: 0.8088 - val_crps_cost_function: 0.8088\n",
      "Epoch 154/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1435 - crps_cost_function: 1.1435 - val_loss: 0.8085 - val_crps_cost_function: 0.8085\n",
      "Epoch 155/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1432 - crps_cost_function: 1.1432 - val_loss: 0.8081 - val_crps_cost_function: 0.8081\n",
      "Epoch 156/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1429 - crps_cost_function: 1.1429 - val_loss: 0.8078 - val_crps_cost_function: 0.8078\n",
      "Epoch 157/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1426 - crps_cost_function: 1.1426 - val_loss: 0.8075 - val_crps_cost_function: 0.8075\n",
      "Epoch 158/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1423 - crps_cost_function: 1.1423 - val_loss: 0.8072 - val_crps_cost_function: 0.8072\n",
      "Epoch 159/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1421 - crps_cost_function: 1.1421 - val_loss: 0.8069 - val_crps_cost_function: 0.8069\n",
      "Epoch 160/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1418 - crps_cost_function: 1.1418 - val_loss: 0.8066 - val_crps_cost_function: 0.8066\n",
      "Epoch 161/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1416 - crps_cost_function: 1.1416 - val_loss: 0.8063 - val_crps_cost_function: 0.8063\n",
      "Epoch 162/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1414 - crps_cost_function: 1.1414 - val_loss: 0.8061 - val_crps_cost_function: 0.8061\n",
      "Epoch 163/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1411 - crps_cost_function: 1.1411 - val_loss: 0.8058 - val_crps_cost_function: 0.8058\n",
      "Epoch 164/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1409 - crps_cost_function: 1.1409 - val_loss: 0.8056 - val_crps_cost_function: 0.8056\n",
      "Epoch 165/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1407 - crps_cost_function: 1.1407 - val_loss: 0.8054 - val_crps_cost_function: 0.8054\n",
      "Epoch 166/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1406 - crps_cost_function: 1.1406 - val_loss: 0.8052 - val_crps_cost_function: 0.8052\n",
      "Epoch 167/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1404 - crps_cost_function: 1.1404 - val_loss: 0.8050 - val_crps_cost_function: 0.8050\n",
      "Epoch 168/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1402 - crps_cost_function: 1.1402 - val_loss: 0.8048 - val_crps_cost_function: 0.8048\n",
      "Epoch 169/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1400 - crps_cost_function: 1.1400 - val_loss: 0.8046 - val_crps_cost_function: 0.8046\n",
      "Epoch 170/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1399 - crps_cost_function: 1.1399 - val_loss: 0.8044 - val_crps_cost_function: 0.8044\n",
      "Epoch 171/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1397 - crps_cost_function: 1.1397 - val_loss: 0.8042 - val_crps_cost_function: 0.8042\n",
      "Epoch 172/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1396 - crps_cost_function: 1.1396 - val_loss: 0.8040 - val_crps_cost_function: 0.8040\n",
      "Epoch 173/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1394 - crps_cost_function: 1.1394 - val_loss: 0.8038 - val_crps_cost_function: 0.8038\n",
      "Epoch 174/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1393 - crps_cost_function: 1.1393 - val_loss: 0.8037 - val_crps_cost_function: 0.8037\n",
      "Epoch 175/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1392 - crps_cost_function: 1.1392 - val_loss: 0.8035 - val_crps_cost_function: 0.8035\n",
      "Epoch 176/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1391 - crps_cost_function: 1.1391 - val_loss: 0.8033 - val_crps_cost_function: 0.8033\n",
      "Epoch 177/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1389 - crps_cost_function: 1.1389 - val_loss: 0.8032 - val_crps_cost_function: 0.8032\n",
      "Epoch 178/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1388 - crps_cost_function: 1.1388 - val_loss: 0.8030 - val_crps_cost_function: 0.8030\n",
      "Epoch 179/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1387 - crps_cost_function: 1.1387 - val_loss: 0.8029 - val_crps_cost_function: 0.8029\n",
      "Epoch 180/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1386 - crps_cost_function: 1.1386 - val_loss: 0.8028 - val_crps_cost_function: 0.8028\n",
      "Epoch 181/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1385 - crps_cost_function: 1.1385 - val_loss: 0.8026 - val_crps_cost_function: 0.8026\n",
      "Epoch 182/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1384 - crps_cost_function: 1.1384 - val_loss: 0.8025 - val_crps_cost_function: 0.8025\n",
      "Epoch 183/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1383 - crps_cost_function: 1.1383 - val_loss: 0.8024 - val_crps_cost_function: 0.8024\n",
      "Epoch 184/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1382 - crps_cost_function: 1.1382 - val_loss: 0.8022 - val_crps_cost_function: 0.8022\n",
      "Epoch 185/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1381 - crps_cost_function: 1.1381 - val_loss: 0.8021 - val_crps_cost_function: 0.8021\n",
      "Epoch 186/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1380 - crps_cost_function: 1.1380 - val_loss: 0.8020 - val_crps_cost_function: 0.8020\n",
      "Epoch 187/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1379 - crps_cost_function: 1.1379 - val_loss: 0.8019 - val_crps_cost_function: 0.8019\n",
      "Epoch 188/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1378 - crps_cost_function: 1.1378 - val_loss: 0.8018 - val_crps_cost_function: 0.8018\n",
      "Epoch 189/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1378 - crps_cost_function: 1.1378 - val_loss: 0.8016 - val_crps_cost_function: 0.8016\n",
      "Epoch 190/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1377 - crps_cost_function: 1.1377 - val_loss: 0.8015 - val_crps_cost_function: 0.8015\n",
      "Epoch 191/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1376 - crps_cost_function: 1.1376 - val_loss: 0.8014 - val_crps_cost_function: 0.8014\n",
      "Epoch 192/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1375 - crps_cost_function: 1.1375 - val_loss: 0.8013 - val_crps_cost_function: 0.8013\n",
      "Epoch 193/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1374 - crps_cost_function: 1.1374 - val_loss: 0.8012 - val_crps_cost_function: 0.8012\n",
      "Epoch 194/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1374 - crps_cost_function: 1.1374 - val_loss: 0.8011 - val_crps_cost_function: 0.8011\n",
      "Epoch 195/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1373 - crps_cost_function: 1.1373 - val_loss: 0.8010 - val_crps_cost_function: 0.8010\n",
      "Epoch 196/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1372 - crps_cost_function: 1.1372 - val_loss: 0.8009 - val_crps_cost_function: 0.8009\n",
      "Epoch 197/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1372 - crps_cost_function: 1.1372 - val_loss: 0.8008 - val_crps_cost_function: 0.8008\n",
      "Epoch 198/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1371 - crps_cost_function: 1.1371 - val_loss: 0.8007 - val_crps_cost_function: 0.8007\n",
      "Epoch 199/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1370 - crps_cost_function: 1.1370 - val_loss: 0.8006 - val_crps_cost_function: 0.8006\n",
      "Epoch 200/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1370 - crps_cost_function: 1.1370 - val_loss: 0.8005 - val_crps_cost_function: 0.8005\n",
      "Epoch 201/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1369 - crps_cost_function: 1.1369 - val_loss: 0.8004 - val_crps_cost_function: 0.8004\n",
      "Epoch 202/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12619/12619 [==============================] - 0s - loss: 1.1369 - crps_cost_function: 1.1369 - val_loss: 0.8003 - val_crps_cost_function: 0.8003\n",
      "Epoch 203/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1368 - crps_cost_function: 1.1368 - val_loss: 0.8002 - val_crps_cost_function: 0.8002\n",
      "Epoch 204/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1367 - crps_cost_function: 1.1367 - val_loss: 0.8001 - val_crps_cost_function: 0.8001\n",
      "Epoch 205/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1367 - crps_cost_function: 1.1367 - val_loss: 0.8000 - val_crps_cost_function: 0.8000\n",
      "Epoch 206/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1366 - crps_cost_function: 1.1366 - val_loss: 0.8000 - val_crps_cost_function: 0.8000\n",
      "Epoch 207/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1366 - crps_cost_function: 1.1366 - val_loss: 0.7999 - val_crps_cost_function: 0.7999\n",
      "Epoch 208/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1365 - crps_cost_function: 1.1365 - val_loss: 0.7998 - val_crps_cost_function: 0.7998\n",
      "Epoch 209/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1365 - crps_cost_function: 1.1365 - val_loss: 0.7997 - val_crps_cost_function: 0.7997\n",
      "Epoch 210/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1364 - crps_cost_function: 1.1364 - val_loss: 0.7996 - val_crps_cost_function: 0.7996\n",
      "Epoch 211/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1363 - crps_cost_function: 1.1363 - val_loss: 0.7995 - val_crps_cost_function: 0.7995\n",
      "Epoch 212/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1363 - crps_cost_function: 1.1363 - val_loss: 0.7994 - val_crps_cost_function: 0.7994\n",
      "Epoch 213/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1362 - crps_cost_function: 1.1362 - val_loss: 0.7994 - val_crps_cost_function: 0.7994\n",
      "Epoch 214/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1362 - crps_cost_function: 1.1362 - val_loss: 0.7993 - val_crps_cost_function: 0.7993\n",
      "Epoch 215/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1361 - crps_cost_function: 1.1361 - val_loss: 0.7992 - val_crps_cost_function: 0.7992\n",
      "Epoch 216/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1361 - crps_cost_function: 1.1361 - val_loss: 0.7991 - val_crps_cost_function: 0.7991\n",
      "Epoch 217/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1360 - crps_cost_function: 1.1360 - val_loss: 0.7990 - val_crps_cost_function: 0.7990\n",
      "Epoch 218/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1360 - crps_cost_function: 1.1360 - val_loss: 0.7990 - val_crps_cost_function: 0.7990\n",
      "Epoch 219/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1359 - crps_cost_function: 1.1359 - val_loss: 0.7989 - val_crps_cost_function: 0.7989\n",
      "Epoch 220/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1359 - crps_cost_function: 1.1359 - val_loss: 0.7988 - val_crps_cost_function: 0.7988\n",
      "Epoch 221/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1359 - crps_cost_function: 1.1359 - val_loss: 0.7987 - val_crps_cost_function: 0.7987\n",
      "Epoch 222/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1358 - crps_cost_function: 1.1358 - val_loss: 0.7986 - val_crps_cost_function: 0.7986\n",
      "Epoch 223/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1358 - crps_cost_function: 1.1358 - val_loss: 0.7986 - val_crps_cost_function: 0.7986\n",
      "Epoch 224/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1357 - crps_cost_function: 1.1357 - val_loss: 0.7985 - val_crps_cost_function: 0.7985\n",
      "Epoch 225/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1357 - crps_cost_function: 1.1357 - val_loss: 0.7984 - val_crps_cost_function: 0.7984\n",
      "Epoch 226/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1356 - crps_cost_function: 1.1356 - val_loss: 0.7983 - val_crps_cost_function: 0.7983\n",
      "Epoch 227/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1356 - crps_cost_function: 1.1356 - val_loss: 0.7983 - val_crps_cost_function: 0.7983\n",
      "Epoch 228/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1355 - crps_cost_function: 1.1355 - val_loss: 0.7982 - val_crps_cost_function: 0.7982\n",
      "Epoch 229/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1355 - crps_cost_function: 1.1355 - val_loss: 0.7981 - val_crps_cost_function: 0.7981\n",
      "Epoch 230/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1355 - crps_cost_function: 1.1355 - val_loss: 0.7980 - val_crps_cost_function: 0.7980\n",
      "Epoch 231/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1354 - crps_cost_function: 1.1354 - val_loss: 0.7980 - val_crps_cost_function: 0.7980\n",
      "Epoch 232/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1354 - crps_cost_function: 1.1354 - val_loss: 0.7979 - val_crps_cost_function: 0.7979\n",
      "Epoch 233/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1353 - crps_cost_function: 1.1353 - val_loss: 0.7978 - val_crps_cost_function: 0.7978\n",
      "Epoch 234/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1353 - crps_cost_function: 1.1353 - val_loss: 0.7978 - val_crps_cost_function: 0.7978\n",
      "Epoch 235/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1353 - crps_cost_function: 1.1353 - val_loss: 0.7977 - val_crps_cost_function: 0.7977\n",
      "Epoch 236/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1352 - crps_cost_function: 1.1352 - val_loss: 0.7976 - val_crps_cost_function: 0.7976\n",
      "Epoch 237/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1352 - crps_cost_function: 1.1352 - val_loss: 0.7975 - val_crps_cost_function: 0.7975\n",
      "Epoch 238/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1351 - crps_cost_function: 1.1351 - val_loss: 0.7975 - val_crps_cost_function: 0.7975\n",
      "Epoch 239/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1351 - crps_cost_function: 1.1351 - val_loss: 0.7974 - val_crps_cost_function: 0.7974\n",
      "Epoch 240/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1351 - crps_cost_function: 1.1351 - val_loss: 0.7973 - val_crps_cost_function: 0.7973\n",
      "Epoch 241/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1350 - crps_cost_function: 1.1350 - val_loss: 0.7973 - val_crps_cost_function: 0.7973\n",
      "Epoch 242/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1350 - crps_cost_function: 1.1350 - val_loss: 0.7972 - val_crps_cost_function: 0.7972\n",
      "Epoch 243/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1349 - crps_cost_function: 1.1349 - val_loss: 0.7971 - val_crps_cost_function: 0.7971\n",
      "Epoch 244/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1349 - crps_cost_function: 1.1349 - val_loss: 0.7971 - val_crps_cost_function: 0.7971\n",
      "Epoch 245/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1349 - crps_cost_function: 1.1349 - val_loss: 0.7970 - val_crps_cost_function: 0.7970\n",
      "Epoch 246/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1348 - crps_cost_function: 1.1348 - val_loss: 0.7969 - val_crps_cost_function: 0.7969\n",
      "Epoch 247/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1348 - crps_cost_function: 1.1348 - val_loss: 0.7969 - val_crps_cost_function: 0.7969\n",
      "Epoch 248/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1347 - crps_cost_function: 1.1347 - val_loss: 0.7968 - val_crps_cost_function: 0.7968\n",
      "Epoch 249/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1347 - crps_cost_function: 1.1347 - val_loss: 0.7967 - val_crps_cost_function: 0.7967\n",
      "Epoch 250/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1347 - crps_cost_function: 1.1347 - val_loss: 0.7967 - val_crps_cost_function: 0.7967\n",
      "Epoch 251/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1346 - crps_cost_function: 1.1346 - val_loss: 0.7966 - val_crps_cost_function: 0.7966\n",
      "Epoch 252/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1346 - crps_cost_function: 1.1346 - val_loss: 0.7965 - val_crps_cost_function: 0.7965\n",
      "Epoch 253/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1346 - crps_cost_function: 1.1346 - val_loss: 0.7965 - val_crps_cost_function: 0.7965\n",
      "Epoch 254/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1345 - crps_cost_function: 1.1345 - val_loss: 0.7964 - val_crps_cost_function: 0.7964\n",
      "Epoch 255/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1345 - crps_cost_function: 1.1345 - val_loss: 0.7963 - val_crps_cost_function: 0.7963\n",
      "Epoch 256/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1345 - crps_cost_function: 1.1345 - val_loss: 0.7963 - val_crps_cost_function: 0.7963\n",
      "Epoch 257/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1344 - crps_cost_function: 1.1344 - val_loss: 0.7962 - val_crps_cost_function: 0.7962\n",
      "Epoch 258/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1344 - crps_cost_function: 1.1344 - val_loss: 0.7961 - val_crps_cost_function: 0.7961\n",
      "Epoch 259/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1344 - crps_cost_function: 1.1344 - val_loss: 0.7961 - val_crps_cost_function: 0.7961\n",
      "Epoch 260/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1343 - crps_cost_function: 1.1343 - val_loss: 0.7960 - val_crps_cost_function: 0.7960\n",
      "Epoch 261/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1343 - crps_cost_function: 1.1343 - val_loss: 0.7959 - val_crps_cost_function: 0.7959\n",
      "Epoch 262/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1342 - crps_cost_function: 1.1342 - val_loss: 0.7959 - val_crps_cost_function: 0.7959\n",
      "Epoch 263/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1342 - crps_cost_function: 1.1342 - val_loss: 0.7958 - val_crps_cost_function: 0.7958\n",
      "Epoch 264/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1342 - crps_cost_function: 1.1342 - val_loss: 0.7958 - val_crps_cost_function: 0.7958\n",
      "Epoch 265/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1341 - crps_cost_function: 1.1341 - val_loss: 0.7957 - val_crps_cost_function: 0.7957\n",
      "Epoch 266/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1341 - crps_cost_function: 1.1341 - val_loss: 0.7956 - val_crps_cost_function: 0.7956\n",
      "Epoch 267/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1341 - crps_cost_function: 1.1341 - val_loss: 0.7956 - val_crps_cost_function: 0.7956\n",
      "Epoch 268/1000\n",
      "12619/12619 [==============================] - 0s - loss: 1.1340 - crps_cost_function: 1.1340 - val_loss: 0.7955 - val_crps_cost_function: 0.7955\n"
     ]
    }
   ],
   "source": [
    "model_keras.fit([tfc_mean_train, tfc_std_train], tobs_train, epochs=steps_max, batch_size=batch_size,\n",
    "          validation_data=[[tfc_mean_test, tfc_std_test], tobs_test], verbose=0,\n",
    "          callbacks=[EarlyStopping(monitor='loss', min_delta=early_stopping_delta,\n",
    "                                  patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get train and test CRPS\n",
    "(model_keras.evaluate([tfc_mean_train, tfc_std_train], tobs_train, batch_size, verbose=0), \n",
    " model_keras.evaluate([tfc_mean_test, tfc_std_test], tobs_test, batch_size, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the CRPS is slightly higher than in our theano implementation. This is most likely due to the differences in the early stopping algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing for 2016\n",
    "\n",
    "Same as above with the theano model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_idx_start = return_date_idx(dates, 2016, 1, 1)\n",
    "date_idx_stop = return_date_idx(dates, 2016, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_keras = build_EMOS_Network_keras()\n",
    "opt = SGD(lr=0.1)  \n",
    "model_keras.compile(optimizer=opt, loss=crps_cost_function, \n",
    "                    metrics=[crps_cost_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "Time: 170.37 s\n"
     ]
    }
   ],
   "source": [
    "train_crps_list, valid_crps_list = loop_over_days(\n",
    "    model_keras,\n",
    "    tobs_full, \n",
    "    tfc_full, \n",
    "    date_idx_start, date_idx_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=lr,\n",
    "    model_type='EMOS_Network_keras',\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the keras implementation is quite a bit slower than the pure theano version. This could be due to the overhead of calling model.fit many many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98057245693860451, 0.99911120397520148)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_crps_list), np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very similar to the theano implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 2015, predict 2016\n",
    "\n",
    "Finally, we will train one single model on all of the 2015 data, and then post-process all of 2016 with this one model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfc_mean_train, tfc_std_train, tobs_train, tfc_mean_test, tfc_std_test, tobs_test = \\\n",
    "        get_train_test_data(tobs_full, tfc_full, date_idx_start, window_size=365, fclt=0, \n",
    "                            subtract_std_mean=False, test_plus=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180849,), (181718,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfc_mean_train.shape, tfc_mean_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_keras = build_EMOS_Network_keras()\n",
    "opt = Adam(lr=0.1)  # Adam is a better SGD in a nutshell\n",
    "model_keras.compile(optimizer=opt, loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 181718 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 2.3505 - val_loss: 1.0104\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0107\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0120\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0158\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0694 - val_loss: 1.0093\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0695 - val_loss: 1.0099\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0694 - val_loss: 1.0098\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0695 - val_loss: 1.0121\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0697 - val_loss: 1.0108\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0696 - val_loss: 1.0087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124a514a8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_keras.fit([tfc_mean_train, tfc_std_train], tobs_train, epochs=10, \n",
    "                batch_size=1024, \n",
    "                validation_data=[[tfc_mean_test, tfc_std_test], tobs_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get a 2016 CRPS of 1.01 compared to 1.00 for the 25 day rolling window. Surprisingly little difference. This suggests that the seasonality is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
