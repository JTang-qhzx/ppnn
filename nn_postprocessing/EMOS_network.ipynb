{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Theano-Implementation\" data-toc-modified-id=\"Theano-Implementation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Theano Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Train-for-a-single-day\" data-toc-modified-id=\"Train-for-a-single-day-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Train for a single day</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Train-for-full-period\" data-toc-modified-id=\"Train-for-full-period-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Train for full period</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Keras-implementation\" data-toc-modified-id=\"Keras-implementation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Keras implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Predict-for-one-day\" data-toc-modified-id=\"Predict-for-one-day-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Predict for one day</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/EMOS_network.ipynb#Predict-for-the-whole-period\" data-toc-modified-id=\"Predict-for-the-whole-period-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Predict for the whole period</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMOS Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from importlib import reload\n",
    "import emos_network_theano; reload(emos_network_theano)\n",
    "from  emos_network_theano import EMOS_Network, crps_cost_function\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "DATA_DIR = '/Users/stephanrasp/repositories/ppnn/data/'  # Mac\n",
    "fn = 'data_interpolated.nc'\n",
    "window_size = 25\n",
    "fclt = 48\n",
    "utc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for a single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the full dataset for 00UTC\n",
    "tobs_full, tfc_full, dates = load_nc_data(DATA_DIR + fn, utc=utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3071"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just pick a day\n",
    "date_idx = np.where(dates == datetime(2015, 6, 1, 0, 0))[0][0]\n",
    "date_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the data slice for this particular date\n",
    "tfc_mean_train, tfc_std_train, tobs_train, \\\n",
    "    tfc_mean_test, tfc_std_test, tobs_test = \\\n",
    "        get_train_test_data(tobs_full, tfc_full, date_idx, \n",
    "                            window_size, fclt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some model parameters\n",
    "lr = np.asarray(0.1, dtype='float32')   # The learning rate\n",
    "early_stopping_delta = 1e-4   # How much the CRPS must improve before stopping\n",
    "steps_max = 1000   # How many steps to fit at max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the theano model\n",
    "model_theano = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(1.1621002564583596), array(0.9137559142044587))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for some steps\n",
    "model_theano.fit(tfc_mean_train, tfc_std_train, tobs_train, steps_max, \n",
    "          (tfc_mean_test, tfc_std_test, tobs_test), lr=lr, \n",
    "          early_stopping_delta=early_stopping_delta)\n",
    "# Output is the training CRPS and the test CRPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for full period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 3650)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get start and stop indices\n",
    "date_idx_start = np.where(dates == datetime(2008, 1, 1, 0, 0))[0][0]\n",
    "date_idx_stop = np.where(dates == datetime(2016, 12, 31, 0, 0))[0][0]\n",
    "date_idx_start, date_idx_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_theano = EMOS_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "Time: 207.37 s\n"
     ]
    }
   ],
   "source": [
    "train_crps_list, valid_crps_list = loop_over_days(\n",
    "    model_theano,\n",
    "    tobs_full, \n",
    "    tfc_full, \n",
    "    date_idx_start, date_idx_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=lr,\n",
    "    model_type='EMOS_Network_theano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0558042002237267"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the mean prediction CRPS is\n",
    "np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard EMOS global score (in `standard_postprocessing/emos_global.R`) is 1.0654. So we are doing a little better even, but this might just be chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation\n",
    "\n",
    "First, let's build the same model in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the keras modules\n",
    "# Note that the cost function only works with the theano backend\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the model with Keras' functional API\n",
    "# This is quite a bit easier and shorter than in theano \n",
    "# But it did take me some time to figure out the cost function, etc.\n",
    "def build_EMOS_Network_keras():\n",
    "    mean_in = Input(shape=(1,))\n",
    "    std_in = Input(shape=(1,))\n",
    "    mean_out = Dense(1, activation='linear')(mean_in)\n",
    "    std_out = Dense(1, activation='linear')(std_in)\n",
    "    x = keras.layers.concatenate([mean_out, std_out], axis=1)\n",
    "    return Model(inputs=[mean_in, std_in], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras = build_EMOS_Network_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_41 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_42 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_41 (Dense)                 (None, 1)             2           input_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_42 (Dense)                 (None, 1)             2           input_42[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (None, 2)             0           dense_41[0][0]                   \n",
      "                                                                   dense_42[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model with SGD initilalizer\n",
    "opt = SGD(lr=0.1)  \n",
    "model_keras.compile(optimizer=opt, loss=crps_cost_function, \n",
    "              metrics=[crps_cost_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This way we have the gradient descent on the whole training set just as in theano\n",
    "batch_size = tfc_mean_train.shape[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model_keras.fit([tfc_mean_train, tfc_std_train], tobs_train, epochs=steps_max, batch_size=batch_size,\n",
    "          validation_data=[[tfc_mean_test, tfc_std_test], tobs_test], verbose=0,\n",
    "          callbacks=[EarlyStopping(monitor='loss', min_delta=early_stopping_delta,\n",
    "                                  patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "   64/12343 [..............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2517586752240233, 1.2517586752240233]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_keras.evaluate([tfc_mean_train, tfc_std_train], tobs_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 64/495 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0580770378143314, 1.0580770378143314]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_keras.evaluate([tfc_mean_test, tfc_std_test], tobs_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se we get slightly better results than with the theano model above. This might be due to the differences in the early stopping algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But I did notice something interesting looking at the history\n",
    "valid_loss = h.history['val_loss']\n",
    "train_loss = h.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvH0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ee0a128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_loss, label='valid_loss')\n",
    "plt.plot(train_loss, label='train_loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means we are overfitting a little. How could we avoid this. Let's pick a much longer training window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One year\n",
    "tfc_mean_train, tfc_std_train, tobs_train, \\\n",
    "    tfc_mean_test, tfc_std_test, tobs_test = \\\n",
    "        get_train_test_data(tobs_full, tfc_full, date_idx, \n",
    "                            356, fclt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = tfc_mean_train.shape[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_EMOS_Network_keras()\n",
    "opt = SGD(lr=0.1)  \n",
    "model.compile(optimizer=opt, loss=crps_cost_function, \n",
    "              metrics=[crps_cost_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = model.fit([tfc_mean_train, tfc_std_train], tobs_train, epochs=steps_max, batch_size=batch_size,\n",
    "          validation_data=[[tfc_mean_test, tfc_std_test], tobs_test], verbose=0,\n",
    "          callbacks=[EarlyStopping(monitor='loss', min_delta=early_stopping_delta,\n",
    "                                  patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 64/495 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0561385891911841, 1.0561385891911841]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([tfc_mean_test, tfc_std_test], tobs_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem to change much. Which means that either we have compensating errors or the seasonal information doesn't mean much. Here are some ideas:\n",
    "- Create an actual validation data set from the training data and use it for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for the whole period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_keras = build_EMOS_Network_keras()\n",
    "opt = SGD(lr=0.1)  \n",
    "model_keras.compile(optimizer=opt, loss=crps_cost_function, \n",
    "              metrics=[crps_cost_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "Time: 291.84 s\n"
     ]
    }
   ],
   "source": [
    "train_crps_list, valid_crps_list = loop_over_days(\n",
    "    model_keras,\n",
    "    tobs_full, \n",
    "    tfc_full, \n",
    "    date_idx_start, date_idx_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=lr,\n",
    "    model_type='EMOS_Network_keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the keras implementation is quite a bit slower than the pure theano version. Maybe this is due to the overhead with calling fit many many times. Which one to keep??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0596537571248619, 1.0596537571248619],\n",
       " [1.0665201209369861, 1.0665201209369861],\n",
       " [1.0615908925508049, 1.0615908925508049],\n",
       " [1.0743945832622437, 1.0743945832622437],\n",
       " [1.0879961302893177, 1.0879961302893177],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " [nan, nan],\n",
       " ...]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_crps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_model():\n",
    "    inp = Input(shape=(2,))\n",
    "    x = Dense(2, activation='linear')(inp)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=crps_cost_function, metrics=[crps_cost_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25190, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_train = np.column_stack([tfc_mean_train, tfc_std_train])\n",
    "in_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_test = np.column_stack([tfc_mean_test, tfc_std_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1272660b8>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(in_train, tobs_train, epochs=1000, batch_size=25190, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25190 samples, validate on 506 samples\n",
      "Epoch 1/1\n",
      "25190/25190 [==============================] - 0s - loss: 1.2329 - crps_cost_function: 1.2329 - val_loss: 1.1481 - val_crps_cost_function: 1.1481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126b4a940>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(in_train, tobs_train, epochs=1, batch_size=25190,\n",
    "          validation_data=[in_test, tobs_test], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
