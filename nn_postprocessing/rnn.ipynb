{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Set-up-data\" data-toc-modified-id=\"Set-up-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Set up data</a></span></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#RNNs-with-only-temperature-data\" data-toc-modified-id=\"RNNs-with-only-temperature-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>RNNs with only temperature data</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#RNN-predicting-only-the-last-target\" data-toc-modified-id=\"RNN-predicting-only-the-last-target-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>RNN predicting only the last target</a></span></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Sequence-RNN\" data-toc-modified-id=\"Sequence-RNN-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Sequence RNN</a></span></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Longer-sequence\" data-toc-modified-id=\"Longer-sequence-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Longer sequence</a></span></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Add-dropout\" data-toc-modified-id=\"Add-dropout-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Add dropout</a></span></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Reference-experiment-with-longer-training-set.\" data-toc-modified-id=\"Reference-experiment-with-longer-training-set.-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Reference experiment with longer training set.</a></span></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Sequence-model-with-longer-training-set\" data-toc-modified-id=\"Sequence-model-with-longer-training-set-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Sequence model with longer training set</a></span></li></ul></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Get-additional-variables\" data-toc-modified-id=\"Get-additional-variables-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Get additional variables</a></span></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Combining-with-embeddings\" data-toc-modified-id=\"Combining-with-embeddings-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Combining with embeddings</a></span></li><li><span><a href=\"http://localhost:8887/notebooks/rnn_test.ipynb#Embedding-with-only-temperature\" data-toc-modified-id=\"Embedding-with-only-temperature-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Embedding with only temperature</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "In this notebook we will try out RNNs for our post-processing. The idea here is that there might be some extra information in looking at data from previous time steps.\n",
    "\n",
    "RNNs take quite a long time to train, so I am using a GPU here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anaconda environment: py36_gpu\n",
      "Linux 4.4.0-96-generic\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from importlib import reload\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import crps_loss; reload(crps_loss)\n",
    "from crps_loss import crps_cost_function, crps_cost_function_seq\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, merge, Embedding, Flatten, Dropout, \\\n",
    "    SimpleRNN, LSTM, TimeDistributed, GRU, Dropout, Masking\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this if you want to limit the GPU RAM usage\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "# DATA_DIR = '/Volumes/STICK/data/ppnn_data/'  # Mac\n",
    "DATA_DIR = '/project/meteo/w2w/C7/ppnn_data/'   # LMU\n",
    "results_dir = '../results/'\n",
    "window_size = 25   # Days in rolling window\n",
    "fclt = 48   # Forecast lead time in hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Set up data\n",
    "\n",
    "This is now also done inside the `get_train_test_sets` function. `seq_len` is the number of timesteps (including the one to predict). We will start out with a moderate length of 5 days, training for 2015, predicting for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates, \n",
    "                                          seq_len=seq_len, fill_value=-999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180849, 5, 2), (180849, 5, 1))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.shape, train_set.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The arrays have dimensions [sample, time step, feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RNNs with only temperature data\n",
    "\n",
    "As a comparison. Our simple networks got a train/test loss of around 1.07/1.01.\n",
    "\n",
    "I am using a Gated Recurrent Unit (GRU) as my recurrent layer. LSTM is probably the more common one, but GRU is slightly cheaper and for our simple applications provides similar results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### RNN predicting only the last target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "hidden_nodes = 100   # Number of hidden nodes inside RNN cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes)(inp)\n",
    "x = Dense(2, activation='linear')(x)\n",
    "rnn_model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rnn_model.compile(optimizer=Adam(0.01), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 5, 2)              0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 100)               30900     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 31,102\n",
      "Trainable params: 31,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 4s - loss: 1.8537 - val_loss: 1.0207\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0508 - val_loss: 1.0188\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0394 - val_loss: 1.0122\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0360 - val_loss: 1.0078\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0333 - val_loss: 1.0066\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0287 - val_loss: 1.0079\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0280 - val_loss: 1.0193\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0236 - val_loss: 1.0325\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0190 - val_loss: 1.0185\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 3s - loss: 1.0131 - val_loss: 1.0348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c7d37e7f0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(train_set.features, train_set.targets[:,-1], epochs=10, batch_size=batch_size,\n",
    "              validation_data=(test_set.features, test_set.targets[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we get a better train score and a worse validation score. This indicates overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sequence RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes, return_sequences=True)(inp)\n",
    "x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "seq_rnn_model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 5, 2)              0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 5, 100)            30900     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 2)              202       \n",
      "=================================================================\n",
      "Total params: 31,102\n",
      "Trainable params: 31,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_rnn_model.compile(optimizer=Adam(0.01), loss=crps_cost_function_seq, \n",
    "                      sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_and_valid(model, train_set, test_set, epochs, batch_size, verbose=0, emb=False):\n",
    "    \"\"\"Write our own function to train and validate, \n",
    "    because the keras fit function cannot handle sample weights for training\n",
    "    and validation at the same time.\n",
    "    \"\"\"\n",
    "    train_inp = [train_set.features, train_set.cont_ids] if emb else train_set.features\n",
    "    test_inp = [test_set.features, test_set.cont_ids] if emb else test_set.features\n",
    "    for i in range(epochs):\n",
    "        print('Epoch:', i+1)\n",
    "        t1 = timeit.default_timer()\n",
    "        \n",
    "        h = model.fit(train_inp, train_set.targets, epochs=1, batch_size=batch_size, \n",
    "                      sample_weight=train_set.sample_weights, verbose=verbose)\n",
    "        t2 = timeit.default_timer()\n",
    "        print('Train loss: %.4f - Valid loss: %.4f - Time: %.1fs' % (h.history['loss'][0],  \n",
    "                    model.evaluate(test_inp, test_set.targets, batch_size=10000, \n",
    "                       sample_weight=test_set.sample_weights, verbose=verbose), \n",
    "                    t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 1.0207 - Valid loss: 1.0338\n",
      "Epoch: 2\n",
      "Train loss: 1.0179 - Valid loss: 1.0420\n",
      "Epoch: 3\n",
      "Train loss: 1.0173 - Valid loss: 1.0316\n",
      "Epoch: 4\n",
      "Train loss: 1.0161 - Valid loss: 1.0381\n",
      "Epoch: 5\n",
      "Train loss: 1.0135 - Valid loss: 1.0471\n",
      "Epoch: 6\n",
      "Train loss: 1.0123 - Valid loss: 1.0401\n",
      "Epoch: 7\n",
      "Train loss: 1.0117 - Valid loss: 1.0343\n",
      "Epoch: 8\n",
      "Train loss: 1.0089 - Valid loss: 1.0547\n",
      "Epoch: 9\n",
      "Train loss: 1.0078 - Valid loss: 1.0515\n",
      "Epoch: 10\n",
      "Train loss: 1.0064 - Valid loss: 1.0479\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(seq_rnn_model, train_set, test_set, 10, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Same as with the first RNN above we seem to overfit to the dataset, but maybe not as strongly. Let's now try a more complex model with a longer sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Longer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates, \n",
    "                                          seq_len=seq_len, fill_value=-999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hidden_nodes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes, return_sequences=True)(inp)\n",
    "x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "seq_rnn_model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 20, 2)             0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 20, 200)           121800    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 2)             402       \n",
      "=================================================================\n",
      "Total params: 122,202\n",
      "Trainable params: 122,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_rnn_model.compile(optimizer=Adam(0.01), loss=crps_cost_function_seq, \n",
    "                      sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 1.7379 - Valid loss: 1.0252\n",
      "Epoch: 2\n",
      "Train loss: 1.0267 - Valid loss: 1.0240\n",
      "Epoch: 3\n",
      "Train loss: 1.0123 - Valid loss: 1.0222\n",
      "Epoch: 4\n",
      "Train loss: 0.9968 - Valid loss: 1.0459\n",
      "Epoch: 5\n",
      "Train loss: 0.9759 - Valid loss: 1.0582\n",
      "Epoch: 6\n",
      "Train loss: 0.9524 - Valid loss: 1.0647\n",
      "Epoch: 7\n",
      "Train loss: 0.9270 - Valid loss: 1.0764\n",
      "Epoch: 8\n",
      "Train loss: 0.9082 - Valid loss: 1.0816\n",
      "Epoch: 9\n",
      "Train loss: 0.8912 - Valid loss: 1.1043\n",
      "Epoch: 10\n",
      "Train loss: 0.8779 - Valid loss: 1.0920\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(seq_rnn_model, train_set, test_set, 10, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So again we are overfitting, but maybe there is something to be learned. Let's first add some regularization and then try a longer training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes, return_sequences=True, recurrent_dropout=0.5)(inp)\n",
    "x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "seq_rnn_model = Model(inputs=inp, outputs=x)\n",
    "seq_rnn_model.compile(optimizer=Adam(0.001), loss=crps_cost_function_seq, \n",
    "                      sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 1.0384 - Valid loss: 1.0048\n",
      "Epoch: 2\n",
      "Train loss: 1.0361 - Valid loss: 1.0030\n",
      "Epoch: 3\n",
      "Train loss: 1.0342 - Valid loss: 1.0019\n",
      "Epoch: 4\n",
      "Train loss: 1.0332 - Valid loss: 1.0035\n",
      "Epoch: 5\n",
      "Train loss: 1.0319 - Valid loss: 1.0027\n",
      "Epoch: 6\n",
      "Train loss: 1.0309 - Valid loss: 1.0033\n",
      "Epoch: 7\n",
      "Train loss: 1.0301 - Valid loss: 1.0073\n",
      "Epoch: 8\n",
      "Train loss: 1.0290 - Valid loss: 1.0035\n",
      "Epoch: 9\n",
      "Train loss: 1.0284 - Valid loss: 1.0086\n",
      "Epoch: 10\n",
      "Train loss: 1.0278 - Valid loss: 1.0054\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(seq_rnn_model, train_set, test_set, 10, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So with drop out we get slightly better validation results, but we are still starting to overfit. I think there is a lot of parameter tuning that would be possible with the complexity of the network and so forth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Reference experiment with longer training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 2922 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_dates_long = ['2008-01-01', '2016-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates_long, test_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Copied from fc_network notebook\n",
    "def build_fc_model():\n",
    "    inp = Input(shape=(2,))\n",
    "    x = Dense(2, activation='linear')(inp)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model()\n",
    "fc_model.compile(optimizer=Adam(0.1), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1456977 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "1456977/1456977 [==============================] - 6s - loss: 1.3737 - val_loss: 1.0100\n",
      "Epoch 2/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0626 - val_loss: 1.0109\n",
      "Epoch 3/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0628 - val_loss: 1.0079\n",
      "Epoch 4/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0630 - val_loss: 1.0126\n",
      "Epoch 5/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0630 - val_loss: 1.0109\n",
      "Epoch 6/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0630 - val_loss: 1.0074\n",
      "Epoch 7/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0629 - val_loss: 1.0119\n",
      "Epoch 8/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0630 - val_loss: 1.0130\n",
      "Epoch 9/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0630 - val_loss: 1.0082\n",
      "Epoch 10/10\n",
      "1456977/1456977 [==============================] - 5s - loss: 1.0630 - val_loss: 1.0167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c519fccc0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "             validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Maybe a small improvement. Now let's test our sequence model with a longer training period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Sequence model with longer training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 2922 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates_long, test_dates, \n",
    "                                          seq_len=seq_len, fill_value=-999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_seq_rnn(hidden_nodes, n_features, dropout=0, lr=0.01):\n",
    "    inp = Input(shape=(seq_len, n_features, )) # time step, feature\n",
    "    x = GRU(hidden_nodes, return_sequences=True, recurrent_dropout=dropout)(inp)\n",
    "    x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "    seq_rnn_model = Model(inputs=inp, outputs=x)\n",
    "    seq_rnn_model.compile(optimizer=Adam(lr), loss=crps_cost_function_seq, \n",
    "                          sample_weight_mode=\"temporal\")\n",
    "    return seq_rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(seq_len, 2, )) # time step, feature\n",
    "x = GRU(hidden_nodes, return_sequences=True, recurrent_dropout=0.5)(inp)\n",
    "x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "seq_rnn_model = Model(inputs=inp, outputs=x)\n",
    "seq_rnn_model.compile(optimizer=Adam(0.001), loss=crps_cost_function_seq, \n",
    "                      sample_weight_mode=\"temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 1.0349 - Valid loss: 0.9941\n",
      "Epoch: 2\n",
      "Train loss: 1.0333 - Valid loss: 0.9959\n"
     ]
    }
   ],
   "source": [
    "# This takes several minutes on the GPU\n",
    "# Epoch counter: 7\n",
    "train_and_valid(seq_rnn_model, train_set, test_set, 2, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "aux_dict = OrderedDict()\n",
    "aux_dict['data_aux_geo_interpolated.nc'] = ['orog', \n",
    "                                            'station_alt', \n",
    "                                            'station_lat', \n",
    "                                            'station_lon']\n",
    "aux_dict['data_aux_pl500_interpolated_00UTC.nc'] = ['u_pl500_fc',\n",
    "                                                    'v_pl500_fc',\n",
    "                                                    'gh_pl500_fc']\n",
    "aux_dict['data_aux_pl850_interpolated_00UTC.nc'] = ['u_pl850_fc',\n",
    "                                                    'v_pl850_fc',\n",
    "                                                    'q_pl850_fc']\n",
    "aux_dict['data_aux_surface_interpolated_00UTC.nc'] = ['cape_fc',\n",
    "                                                      'sp_fc',\n",
    "                                                      'tcc_fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "# Start with just one training year\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates, \n",
    "                                          seq_len=seq_len, fill_value=-999., aux_dict=aux_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180849, 20)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.cont_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = train_set.features.shape[-1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_rnn_model = build_seq_rnn(hidden_nodes, n_features, dropout=0.5, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 18s - loss: 0.8329    \n",
      "182218/182218 [==============================] - 6s     \n",
      "Train loss: 0.8329 - Valid loss: 0.9341 - Time: 18.1s\n",
      "Epoch: 2\n",
      "Epoch 1/1\n",
      "180849/180849 [==============================] - 18s - loss: 0.8201    \n",
      "182218/182218 [==============================] - 6s     \n",
      "Train loss: 0.8201 - Valid loss: 0.9315 - Time: 18.1s\n"
     ]
    }
   ],
   "source": [
    "# Epoch counter: 8\n",
    "train_and_valid(seq_rnn_model, train_set, test_set, 2, batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 20, 24)            0         \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 20, 200)           135000    \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 20, 2)             402       \n",
      "=================================================================\n",
      "Total params: 135,402\n",
      "Trainable params: 135,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_seq_rnn_with_embeddings(seq_len, hidden_nodes, n_features, emb_size, max_id, \n",
    "                                  recurrent_dropout=0, dropout=0, lr=0.01):\n",
    "    features_inp = Input(shape=(seq_len, n_features, )) # time step, feature\n",
    "    id_in = Input(shape=(seq_len,))\n",
    "    emb = Embedding(max_id + 1, emb_size)(id_in)\n",
    "    x = GRU(hidden_nodes, return_sequences=True, recurrent_dropout=recurrent_dropout)(features_inp)\n",
    "    x = Concatenate()([x, emb])\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = TimeDistributed(Dense(2, activation='linear'))(x)\n",
    "    model = Model(inputs=[features_inp, id_in], outputs=x)\n",
    "    model.compile(optimizer=Adam(lr), loss=crps_cost_function_seq, \n",
    "                          sample_weight_mode=\"temporal\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 536)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 5\n",
    "max_id = int(np.max([train_set.cont_ids.max(), test_set.cont_ids.max()]))\n",
    "hidden_nodes, max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_rnn = build_seq_rnn_with_embeddings(hidden_nodes, n_features, emb_size, max_id, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_24 (InputLayer)            (None, 20, 24)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_25 (InputLayer)            (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gru_16 (GRU)                     (None, 20, 200)       135000      input_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 20, 5)         2685        input_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 20, 205)       0           gru_16[0][0]                     \n",
      "                                                                   embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 20, 205)       0           concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistrib (None, 20, 2)         412         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 138,097\n",
      "Trainable params: 138,097\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 0.9147 - Valid loss: 0.8970 - Time: 19.6s\n",
      "Epoch: 2\n",
      "Train loss: 0.9016 - Valid loss: 0.8984 - Time: 19.7s\n",
      "Epoch: 3\n",
      "Train loss: 0.8923 - Valid loss: 0.9004 - Time: 19.6s\n",
      "Epoch: 4\n",
      "Train loss: 0.8822 - Valid loss: 0.9023 - Time: 19.6s\n",
      "Epoch: 5\n",
      "Train loss: 0.8737 - Valid loss: 0.8983 - Time: 19.6s\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(emb_rnn, train_set, test_set, 5, batch_size, emb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding with only temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates, \n",
    "                                          seq_len=2, fill_value=-999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_rnn = build_seq_rnn_with_embeddings(2, 10, 2, emb_size, max_id, recurrent_dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_36 (InputLayer)            (None, 2, 2)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_37 (InputLayer)            (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "gru_22 (GRU)                     (None, 2, 10)         390         input_36[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 2, 5)          2685        input_37[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)     (None, 2, 15)         0           gru_22[0][0]                     \n",
      "                                                                   embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 2, 15)         0           concatenate_13[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistrib (None, 2, 2)          32          dropout_7[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3,107\n",
      "Trainable params: 3,107\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 0.9538 - Valid loss: 0.9194 - Time: 1.2s\n",
      "Epoch: 2\n",
      "Train loss: 0.9534 - Valid loss: 0.9233 - Time: 1.1s\n",
      "Epoch: 3\n",
      "Train loss: 0.9541 - Valid loss: 0.9263 - Time: 1.1s\n",
      "Epoch: 4\n",
      "Train loss: 0.9538 - Valid loss: 0.9229 - Time: 1.2s\n",
      "Epoch: 5\n",
      "Train loss: 0.9536 - Valid loss: 0.9233 - Time: 1.1s\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(emb_rnn, train_set, test_set, 5, batch_size, emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 2922 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_dates_long = ['2008-01-01', '2016-01-01']\n",
    "train_set_long, test_set = get_train_test_sets(DATA_DIR, train_dates_long, test_dates, \n",
    "                                          seq_len=seq_len, fill_value=-999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_rnn = build_seq_rnn_with_embeddings(seq_len, 30, 2, emb_size, max_id, recurrent_dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 1.2161 - Valid loss: 0.9024 - Time: 40.9s\n",
      "Epoch: 2\n",
      "Train loss: 0.9281 - Valid loss: 0.8962 - Time: 38.3s\n",
      "Epoch: 3\n",
      "Train loss: 0.9255 - Valid loss: 0.8949 - Time: 38.3s\n",
      "Epoch: 4\n",
      "Train loss: 0.9239 - Valid loss: 0.8965 - Time: 38.3s\n",
      "Epoch: 5\n",
      "Train loss: 0.9227 - Valid loss: 0.8954 - Time: 38.3s\n",
      "Epoch: 6\n",
      "Train loss: 0.9216 - Valid loss: 0.9010 - Time: 38.3s\n",
      "Epoch: 7\n",
      "Train loss: 0.9210 - Valid loss: 0.9060 - Time: 38.3s\n",
      "Epoch: 8\n",
      "Train loss: 0.9205 - Valid loss: 0.8914 - Time: 38.2s\n",
      "Epoch: 9\n",
      "Train loss: 0.9198 - Valid loss: 0.8937 - Time: 38.3s\n",
      "Epoch: 10\n",
      "Train loss: 0.9193 - Valid loss: 0.8931 - Time: 38.2s\n",
      "Epoch: 11\n",
      "Train loss: 0.9191 - Valid loss: 0.8955 - Time: 38.2s\n",
      "Epoch: 12\n",
      "Train loss: 0.9188 - Valid loss: 0.9047 - Time: 38.2s\n",
      "Epoch: 13\n",
      "Train loss: 0.9186 - Valid loss: 0.8932 - Time: 38.2s\n",
      "Epoch: 14\n",
      "Train loss: 0.9184 - Valid loss: 0.8945 - Time: 38.2s\n",
      "Epoch: 15\n",
      "Train loss: 0.9183 - Valid loss: 0.8993 - Time: 38.2s\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(emb_rnn, train_set_long, test_set, 15, batch_size, emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
