{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "data = pd.read_feather(path = \"/home/sebastian/Downloads/data_RL18.feather\")\n",
    "data.station = pd.to_numeric(data.station, downcast = 'integer')\n",
    "\n",
    "# drop soil moisture predictions due to missing values\n",
    "data = data.drop(['sm_mean', 'sm_var'], axis=1)\n",
    "\n",
    "# split into train and test data\n",
    "eval_start = 1626724\n",
    "train_end = 1626723\n",
    "\n",
    "train_features_raw = data.iloc[:train_end,3:42].to_numpy()\n",
    "train_targets = data.iloc[:train_end,2].to_numpy()\n",
    "train_IDs = data.iloc[:train_end,1].to_numpy()\n",
    "\n",
    "test_features_raw = data.iloc[eval_start:,3:42].to_numpy()\n",
    "test_targets = data.iloc[eval_start:,2].to_numpy()\n",
    "test_IDs = data.iloc[eval_start:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "\n",
    "def normalize(data, method=None, shift=None, scale=None):\n",
    "    result = np.zeros(data.shape)\n",
    "    if method == \"MAX\":\n",
    "        scale = np.max(data, axis=0)\n",
    "        shift = np.zeros(scale.shape)\n",
    "    for index in range(len(data[0])):\n",
    "        result[:,index] = (data[:,index] - shift[index]) / scale[index]\n",
    "    return result, shift, scale\n",
    "\n",
    "train_features, train_shift, train_scale = normalize(train_features_raw[:,:], method=\"MAX\")\n",
    "\n",
    "test_features = normalize(test_features_raw[:,:], shift=train_shift, scale=train_scale)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for NN models\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, merge, Embedding, Flatten, Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def crps_cost_function(y_true, y_pred, theano=False):\n",
    "    \"\"\"Compute the CRPS cost function for a normal distribution defined by\n",
    "    the mean and standard deviation.\n",
    "\n",
    "    Code inspired by Kai Polsterer (HITS).\n",
    "\n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Tensor containing predictions: [mean, std]\n",
    "        theano: Set to true if using this with pure theano.\n",
    "\n",
    "    Returns:\n",
    "        mean_crps: Scalar with mean CRPS over batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Split input\n",
    "    mu = y_pred[:, 0]\n",
    "    sigma = y_pred[:, 1]\n",
    "    # Ugly workaround for different tensor allocation in keras and theano\n",
    "    if not theano:\n",
    "        y_true = y_true[:, 0]   # Need to also get rid of axis 1 to match!\n",
    "\n",
    "    # To stop sigma from becoming negative we first have to \n",
    "    # convert it the the variance and then take the square\n",
    "    # root again. \n",
    "    var = K.square(sigma)\n",
    "    # The following three variables are just for convenience\n",
    "    loc = (y_true - mu) / K.sqrt(var)\n",
    "    phi = 1.0 / np.sqrt(2.0 * np.pi) * K.exp(-K.square(loc) / 2.0)\n",
    "    Phi = 0.5 * (1.0 + tf.math.erf(loc / np.sqrt(2.0)))\n",
    "    # First we will compute the crps for each input/target pair\n",
    "    crps =  K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
    "    # Then we take the mean. The cost is now a scalar\n",
    "    return K.mean(crps)\n",
    "\n",
    "def build_emb_model(n_features, n_outputs, hidden_nodes, emb_size, max_id,\n",
    "                    compile=False, optimizer='adam', lr=0.01,\n",
    "                    loss=crps_cost_function,\n",
    "                    activation='relu', reg=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        n_features: Number of features\n",
    "        n_outputs: Number of outputs\n",
    "        hidden_nodes: int or list of hidden nodes\n",
    "        emb_size: Embedding size\n",
    "        max_id: Max embedding ID\n",
    "        compile: If true, compile model\n",
    "        optimizer: Name of optimizer\n",
    "        lr: learning rate\n",
    "        loss: loss function\n",
    "        activation: Activation function for hidden layer\n",
    "\n",
    "    Returns:\n",
    "        model: Keras model\n",
    "    \"\"\"\n",
    "    if type(hidden_nodes) is not list:\n",
    "        hidden_nodes = [hidden_nodes]\n",
    "\n",
    "    features_in = Input(shape=(n_features,))\n",
    "    id_in = Input(shape=(1,))\n",
    "    emb = Embedding(max_id + 1, emb_size)(id_in)\n",
    "    emb = Flatten()(emb)\n",
    "    x = Concatenate()([features_in, emb])\n",
    "    for h in hidden_nodes:\n",
    "        x = Dense(h, activation=activation, kernel_regularizer=reg)(x)\n",
    "    x = Dense(n_outputs, activation='linear', kernel_regularizer=reg)(x)\n",
    "    model = Model(inputs=[features_in, id_in], outputs=x)\n",
    "\n",
    "    if compile:\n",
    "        opt = keras.optimizers.Adam(lr=lr)\n",
    "        model.compile(optimizer=opt, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29842c8274a84aef87d6966265560a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training multiple models in a loop\n",
    "\n",
    "emb_size = 2\n",
    "max_id = int(np.max([train_IDs.max(), test_IDs.max()]))\n",
    "n_features = train_features.shape[1]\n",
    "n_outputs = 2\n",
    "\n",
    "nreps = 10\n",
    "trn_scores = []\n",
    "test_scores = []\n",
    "preds = []\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(nreps)):\n",
    "    \n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    features_in = Input(shape=(n_features,))\n",
    "    id_in = Input(shape=(1,))\n",
    "    emb = Embedding(max_id + 1, emb_size)(id_in)\n",
    "    emb = Flatten()(emb)\n",
    "    x = Concatenate()([features_in, emb])\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(n_outputs, activation='linear')(x)\n",
    "    nn_aux_emb = Model(inputs=[features_in, id_in], outputs=x)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.002)\n",
    "    nn_aux_emb.compile(optimizer=opt, loss=crps_cost_function)\n",
    "    \n",
    "    nn_aux_emb.fit([train_features, train_IDs], train_targets, epochs=15, batch_size=4096, verbose=0)   \n",
    "    \n",
    "    trn_scores.append(nn_aux_emb.evaluate([train_features, train_IDs], train_targets, 4096, verbose=0))\n",
    "    test_scores.append(nn_aux_emb.evaluate([test_features, test_IDs], test_targets, 4096, verbose=0))\n",
    "    preds.append(nn_aux_emb.predict([test_features, test_IDs], 4096, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7906655669212341,\n",
       " 0.7881921529769897,\n",
       " 0.7917828559875488,\n",
       " 0.8032492399215698,\n",
       " 0.8050575256347656,\n",
       " 0.7835638523101807,\n",
       " 0.8066595792770386,\n",
       " 0.7818636298179626,\n",
       " 0.7930356860160828,\n",
       " 0.7871763706207275]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble test score = 0.7803375211457555\n"
     ]
    }
   ],
   "source": [
    "# evaluate ensemble of models\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def crps_normal(mu, sigma, y):\n",
    "    \"\"\"\n",
    "    Compute CRPS for a Gaussian distribution. \n",
    "    \"\"\"\n",
    "    # Make sure sigma is positive\n",
    "    sigma = np.abs(sigma)\n",
    "    loc = (y - mu) / sigma\n",
    "    crps = sigma * (loc * (2 * norm.cdf(loc) - 1) + \n",
    "                    2 * norm.pdf(loc) - 1. / np.sqrt(np.pi))\n",
    "    return crps\n",
    "\n",
    "preds = np.array(preds)\n",
    "preds[:, :, 1] = np.abs(preds[:, :, 1]) # Make sure std is positive\n",
    "mean_preds = np.mean(preds, 0)\n",
    "ens_score = crps_normal(mean_preds[:, 0], mean_preds[:, 1], test_targets).mean()\n",
    "print(f'Ensemble test score = {ens_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
